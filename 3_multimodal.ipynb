{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Multi-Modal Transformer Implementation (Inline)\n",
    "\n",
    "### Introduction: Extending the Transformer for Vision and Language\n",
    "\n",
    "**Recap: The Text-Only Transformer**\n",
    "\n",
    "In `transformer.ipynb`, we built and trained a Decoder-only Transformer model capable of generating text character by character. It learned patterns in text sequences using components like token embeddings, positional encoding, masked multi-head self-attention, and feed-forward networks. We saved the trained parameters (weights and configuration) in `saved_models/transformer_model.pt`.\n",
    "\n",
    "**The Multi-Modal Challenge**\n",
    "\n",
    "Our goal now is to extend this model to understand and reason about *both* visual information (images) and text prompts simultaneously. We want to build a system that can, for instance, look at an image and answer a question about it or describe it based on a textual prompt.\n",
    "\n",
    "**Our Approach: Vision Feature Extraction + Fusion**\n",
    "\n",
    "1.  **Load Pre-trained Text Model:** We will start by loading the weights and configuration from our previously trained character-level Transformer. This gives us a head start on text processing.\n",
    "2.  **Vision Feature Extraction:** We need a way to convert an image into a numerical representation that the Transformer can understand. We will use a pre-trained Convolutional Neural Network (CNN), specifically ResNet-18 provided by `torchvision`, as a feature extractor. We will remove its final classification layer and use the output of its penultimate layer as a fixed-size feature vector representing the image content.\n",
    "3.  **Feature Projection:** The image feature vector will likely have a different dimension than our Transformer's `d_model`. We will add a learnable linear layer to project the image features into the `d_model` space.\n",
    "4.  **Modality Fusion (Simple Concatenation):** We will represent the image conceptually as one or more special \"image tokens\" at the beginning of the input sequence. We'll add a special `<IMG>` token to our vocabulary. The projected image features will be associated with this `<IMG>` token embedding. The input to the Transformer will then be a sequence starting with the image representation followed by the tokenized text prompt.\n",
    "5.  **Training:** We will fine-tune the entire model (or parts of it) on a small dataset of (Image, Prompt, Response) triplets. The model learns to attend to both the image features and the prompt text to generate the correct response.\n",
    "6.  **Inference/Generation:** Given a new image and prompt, we will extract image features, combine them with the tokenized prompt, and use the trained model to autoregressively generate the text response character by character, similar to the text-only generation process.\n",
    "\n",
    "**Inline Implementation Style**\n",
    "\n",
    "As requested, we will implement this entire process inline, step-by-step, with detailed explanations for each small code block, avoiding functions and classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setup - Libraries, Loading, Data, Vision Model\n",
    "\n",
    "**Goal:** Prepare the environment, load the pre-trained text model, define sample multi-modal data, and set up the vision feature extractor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.1: Import Libraries\n",
    "\n",
    "**Theory:** We need `torch` and its submodules (`nn`, `F`, `optim`). We also need `torchvision` for the pre-trained ResNet model and image transformations, `PIL` (Pillow) for loading images, `math` for calculations, and `os` for path handling."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu118\n",
      "Torchvision version: 0.21.0+cu118\n",
      "Libraries imported.\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image\n",
    "import math\n",
    "import os\n",
    "import numpy as np # For creating dummy images\n",
    "\n",
    "# For reproducibility (optional, but good practice)\n",
    "torch.manual_seed(42) # Use a different seed for variation\n",
    "np.random.seed(42)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(\"Libraries imported.\")\n",
    "\n",
    "# --- Device Configuration ---\n",
    "# Theory: Set the device (GPU if available, else CPU) for tensor operations.\n",
    "# This ensures that models and data are processed on the same hardware.\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.2: Load Pre-trained Text Model State\n",
    "\n",
    "**Theory:** Load the saved state dictionary from `transformer_model.pt`. This dictionary contains the weights of the previously trained character-level Transformer, its configuration (hyperparameters like `d_model`, `n_layers`, etc.), and the character tokenizer mappings (`char_to_int`, `int_to_char`). We extract these components to rebuild and extend the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0.2: Loading pre-trained text model state...\n",
      "Loaded state dictionary from 'saved_models/transformer_model.pt'.\n",
      "Extracted model configuration and tokenizer:\n",
      "  Loaded vocab_size: 36\n",
      "  d_model: 64\n",
      "  n_layers: 3\n",
      "  n_heads: 4\n",
      "  d_ff: 256\n",
      "  Loaded block_size: 32\n"
     ]
    }
   ],
   "source": [
    "# --- Load Saved Text Model State ---\n",
    "# Theory: Load the dictionary saved from the previous notebook.\n",
    "# This contains model weights, configuration, and tokenizer info.\n",
    "print(\"\\nStep 0.2: Loading pre-trained text model state...\")\n",
    "model_load_path = 'saved_models/transformer_model.pt'\n",
    "if not os.path.exists(model_load_path):\n",
    "    raise FileNotFoundError(f\"Error: Model file not found at {model_load_path}. Please ensure 'transformer2.ipynb' was run and saved the model.\")\n",
    "\n",
    "loaded_state_dict = torch.load(model_load_path, map_location=device)\n",
    "print(f\"Loaded state dictionary from '{model_load_path}'.\")\n",
    "\n",
    "# --- Extract Config and Tokenizer ---\n",
    "# Theory: Retrieve the hyperparameters and tokenizer mappings saved within the state dict.\n",
    "# These are essential for rebuilding the model structure and processing text.\n",
    "config = loaded_state_dict['config']\n",
    "loaded_vocab_size = config['vocab_size']\n",
    "d_model = config['d_model']\n",
    "n_heads = config['n_heads']\n",
    "n_layers = config['n_layers']\n",
    "d_ff = config['d_ff']\n",
    "loaded_block_size = config['block_size'] # Max sequence length for text model\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "char_to_int = loaded_state_dict['tokenizer']['char_to_int']\n",
    "int_to_char = loaded_state_dict['tokenizer']['int_to_char']\n",
    "\n",
    "print(\"Extracted model configuration and tokenizer:\")\n",
    "print(f\"  Loaded vocab_size: {loaded_vocab_size}\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  n_layers: {n_layers}\")\n",
    "print(f\"  n_heads: {n_heads}\")\n",
    "print(f\"  d_ff: {d_ff}\")\n",
    "print(f\"  Loaded block_size: {loaded_block_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.3: Define Special Tokens and Update Vocabulary\n",
    "\n",
    "**Theory:** For our multi-modal setup, we need special tokens:\n",
    "*   `<IMG>`: A placeholder token representing the image input in the sequence.\n",
    "*   `<PAD>`: A token used for padding sequences to a fixed length (`block_size`). Padding is necessary for batch processing.\n",
    "*   `<EOS>`: (Optional but good practice) An End-of-Sentence token to signal the model when to stop generation.\n",
    "We add these to our existing character vocabulary and update the `vocab_size` and mappings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0.3: Defining special tokens and updating vocabulary...\n",
      "Added special tokens: ['<IMG>', '<PAD>', '<EOS>']\n",
      "Updated vocabulary size: 39\n",
      "PAD token ID: 37\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 0.3: Defining special tokens and updating vocabulary...\")\n",
    "\n",
    "# --- Define Special Tokens ---\n",
    "img_token = \"<IMG>\"\n",
    "pad_token = \"<PAD>\"\n",
    "eos_token = \"<EOS>\" # End-of-Sentence/Sequence\n",
    "special_tokens = [img_token, pad_token, eos_token]\n",
    "\n",
    "# --- Add Special Tokens to Vocabulary ---\n",
    "# Theory: Integrate the new special tokens into the existing character mappings.\n",
    "# We assign new unique integer IDs to them. The vocab_size increases.\n",
    "current_vocab_size = loaded_vocab_size\n",
    "for token in special_tokens:\n",
    "    if token not in char_to_int:\n",
    "        char_to_int[token] = current_vocab_size\n",
    "        int_to_char[current_vocab_size] = token\n",
    "        current_vocab_size += 1\n",
    "\n",
    "# Update vocab_size\n",
    "vocab_size = current_vocab_size\n",
    "pad_token_id = char_to_int[pad_token] # Store the ID for later use\n",
    "\n",
    "print(f\"Added special tokens: {special_tokens}\")\n",
    "print(f\"Updated vocabulary size: {vocab_size}\")\n",
    "print(f\"PAD token ID: {pad_token_id}\")\n",
    "# print(f\"Updated char_to_int mapping includes: { {k: char_to_int[k] for k in special_tokens} }\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.4: Define Sample Multi-Modal Data\n",
    "\n",
    "**Theory:** Create a small, synthetic dataset of (image, prompt, response) triplets. For simplicity, we'll generate dummy images (e.g., solid colors) using PIL/Numpy and pair them with descriptive prompts and answers. In a real scenario, this would be a large dataset like COCO Captions, VQAv2, etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0.4: Defining sample multi-modal data...\n",
      "Created dummy images in 'sample_multimodal_data'.\n",
      "Defined 6 sample multi-modal data points.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 0.4: Defining sample multi-modal data...\")\n",
    "\n",
    "# --- Create Dummy Image Files ---\n",
    "# Theory: Generate simple images (e.g., solid colors) and save them.\n",
    "# This avoids needing external image datasets for this inline example.\n",
    "sample_data_dir = \"sample_multimodal_data\"\n",
    "os.makedirs(sample_data_dir, exist_ok=True)\n",
    "\n",
    "image_paths = {\n",
    "    \"red\": os.path.join(sample_data_dir, \"red_square.png\"),\n",
    "    \"blue\": os.path.join(sample_data_dir, \"blue_square.png\"),\n",
    "    \"green\": os.path.join(sample_data_dir, \"green_circle.png\") # Let's add a shape difference\n",
    "}\n",
    "\n",
    "# Create Red Square\n",
    "img_red = Image.new('RGB', (64, 64), color = 'red')\n",
    "img_red.save(image_paths[\"red\"])\n",
    "# Create Blue Square\n",
    "img_blue = Image.new('RGB', (64, 64), color = 'blue')\n",
    "img_blue.save(image_paths[\"blue\"])\n",
    "# Create Green Circle (approximate with PIL draw)\n",
    "img_green = Image.new('RGB', (64, 64), color = 'white')\n",
    "from PIL import ImageDraw\n",
    "draw = ImageDraw.Draw(img_green)\n",
    "draw.ellipse((4, 4, 60, 60), fill='green', outline='green')\n",
    "img_green.save(image_paths[\"green\"])\n",
    "\n",
    "print(f\"Created dummy images in '{sample_data_dir}'.\")\n",
    "\n",
    "# --- Define Data Triplets ---\n",
    "# Theory: Create pairs of (image_path, text_prompt, text_response).\n",
    "# The prompts ask about the image, and responses provide the answer.\n",
    "# Added <EOS> token to the end of responses.\n",
    "sample_training_data = [\n",
    "    {\"image_path\": image_paths[\"red\"], \"prompt\": \"What color is the shape?\", \"response\": \"red.\" + eos_token},\n",
    "    {\"image_path\": image_paths[\"blue\"], \"prompt\": \"Describe the image.\", \"response\": \"a blue square.\" + eos_token},\n",
    "    {\"image_path\": image_paths[\"green\"], \"prompt\": \"What shape is shown?\", \"response\": \"a green circle.\" + eos_token},\n",
    "    {\"image_path\": image_paths[\"red\"], \"prompt\": \"Is it a circle?\", \"response\": \"no, it is a square.\" + eos_token},\n",
    "    {\"image_path\": image_paths[\"blue\"], \"prompt\": \"What is the main color?\", \"response\": \"blue.\" + eos_token},\n",
    "    {\"image_path\": image_paths[\"green\"], \"prompt\": \"Describe this.\", \"response\": \"a circle, it is green.\" + eos_token}\n",
    "]\n",
    "\n",
    "num_samples = len(sample_training_data)\n",
    "print(f\"Defined {num_samples} sample multi-modal data points.\")\n",
    "# print(f\"Sample 0: {sample_training_data[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.5: Load Pre-trained Vision Model (Feature Extractor)\n",
    "\n",
    "**Theory:** Load a pre-trained ResNet-18 model from `torchvision`. We remove its final fully connected layer (`fc`), which was originally used for ImageNet classification. The output of the layer *before* `fc` (the adaptive average pooling layer) will serve as our image feature vector. We set the model to evaluation mode (`eval()`) and disable gradients as we will *not* be fine-tuning the vision backbone in this simple example (only the projection layer and the text transformer)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0.5: Loading pre-trained vision model (ResNet-18)...\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded ResNet-18 feature extractor.\n",
      "  Output feature dimension: 512\n",
      "  Vision model set to evaluation mode on device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 0.5: Loading pre-trained vision model (ResNet-18)...\")\n",
    "\n",
    "# --- Load Pre-trained ResNet-18 ---\n",
    "# Using weights=ResNet18_Weights.DEFAULT for the latest recommended weights\n",
    "vision_model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "\n",
    "# --- Remove Final Classification Layer ---\n",
    "# Theory: We want image features, not class predictions. Access the layer before 'fc'.\n",
    "# In ResNet, the features are output by the adaptive average pooling layer just before 'fc'.\n",
    "# We can replace 'fc' with an identity layer or simply use a hook/modify the forward pass.\n",
    "# Easiest inline: Replace fc with an identity mapping.\n",
    "vision_feature_dim = vision_model.fc.in_features # Get the input dimension of the original fc layer\n",
    "vision_model.fc = nn.Identity() # Replace the classifier\n",
    "\n",
    "# --- Set to Evaluation Mode and Move to Device ---\n",
    "vision_model = vision_model.to(device)\n",
    "vision_model.eval() # Set to evaluation mode (disables dropout, batchnorm updates)\n",
    "\n",
    "print(f\"Loaded ResNet-18 feature extractor.\")\n",
    "print(f\"  Output feature dimension: {vision_feature_dim}\") # Should be 512 for ResNet-18\n",
    "print(f\"  Vision model set to evaluation mode on device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.6: Define Image Transformations\n",
    "\n",
    "**Theory:** Images need to be preprocessed before being fed into the ResNet model. This typically involves resizing them to a standard size (e.g., 224x224 for ResNet), converting them to PyTorch tensors, and normalizing their pixel values using the mean and standard deviation specific to the dataset the model was pre-trained on (ImageNet in this case). `torchvision.transforms` provides these utilities."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0.6: Defining image transformations...\n",
      "Defined image preprocessing pipeline (Resize, Crop, ToTensor, Normalize).\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 0.6: Defining image transformations...\")\n",
    "\n",
    "# --- Define Standard ImageNet Transforms ---\n",
    "# Theory: Use the standard transformations recommended for models pre-trained on ImageNet.\n",
    "# Resize, Center Crop (optional but common), ToTensor, Normalize.\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),            # Resize smaller edge to 256\n",
    "    transforms.CenterCrop(224),        # Crop center 224x224 square\n",
    "    transforms.ToTensor(),             # Convert PIL Image to FloatTensor (0-1 range)\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], # ImageNet mean\n",
    "                         std=[0.229, 0.224, 0.225])   # ImageNet std dev\n",
    "])\n",
    "\n",
    "print(\"Defined image preprocessing pipeline (Resize, Crop, ToTensor, Normalize).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.7: Define New Hyperparameters\n",
    "\n",
    "**Theory:** Define any new hyperparameters specific to the multi-modal setup. We'll set a `block_size` for the combined sequence length (image features + text). It might need to be larger than the original `loaded_block_size` to accommodate both modalities. We also define the number of image \"tokens\" we'll use. For simplicity, we'll use one `<IMG>` token representing the global image feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0.7: Defining new/updated hyperparameters...\n",
      "  Set combined block_size: 64\n",
      "  Using 1 <IMG> token(s) to represent image features.\n",
      "  Updated Training Params: LR=0.0003, BatchSize=4, Epochs=2000\n",
      "  Max sequence length in sample data (approx): 43\n",
      "  Recreated causal mask for new block_size=64\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 0.7: Defining new/updated hyperparameters...\")\n",
    "\n",
    "# --- Multi-Modal Sequence Length ---\n",
    "# Theory: Define the maximum sequence length for the combined image + text input.\n",
    "# Needs to accommodate image tokens + prompt + response (during training).\n",
    "# Let's try a slightly larger size than the original text block size.\n",
    "block_size = 64 # Max length for combined sequence (e.g., 1 IMG + prompt + response)\n",
    "print(f\"  Set combined block_size: {block_size}\")\n",
    "\n",
    "# --- Number of Image Tokens ---\n",
    "# Theory: How many token positions will represent the image? We'll use 1 for simplicity.\n",
    "num_img_tokens = 1\n",
    "print(f\"  Using {num_img_tokens} <IMG> token(s) to represent image features.\")\n",
    "\n",
    "# --- Training Parameters ---\n",
    "# Re-state or adjust training parameters if needed\n",
    "learning_rate = 3e-4 # Keep the same AdamW learning rate\n",
    "batch_size = 4 # Reduce batch size due to potentially larger memory footprint\n",
    "epochs = 2000  # Increase epochs further for multi-modal learning\n",
    "eval_interval = 500\n",
    "\n",
    "print(f\"  Updated Training Params: LR={learning_rate}, BatchSize={batch_size}, Epochs={epochs}\")\n",
    "\n",
    "# Ensure block_size is sufficient\n",
    "min_req_block_size = num_img_tokens + max(len(d[\"prompt\"]) + len(d[\"response\"]) for d in sample_training_data) + 1 # +1 for safety/EOS\n",
    "print(f\"  Max sequence length in sample data (approx): {min_req_block_size}\")\n",
    "if block_size < min_req_block_size:\n",
    "     print(f\"Warning: block_size ({block_size}) might be too small for the longest sample sequence ({min_req_block_size}). Consider increasing it.\")\n",
    "\n",
    "# Recreate the causal mask for the new block_size\n",
    "# Shape: (1, 1, block_size, block_size)\n",
    "causal_mask = torch.tril(torch.ones(block_size, block_size, device=device)).view(1, 1, block_size, block_size)\n",
    "print(f\"  Recreated causal mask for new block_size={block_size}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Data Preparation for Multi-Modal Training\n",
    "\n",
    "**Goal:** Process the sample data: extract image features, tokenize text, combine them into sequences suitable for the Transformer, and prepare batches."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1: Extract Image Features for Sample Data\n",
    "\n",
    "**Theory:** Iterate through our `sample_training_data`. For each unique image path, load the image, apply the defined transformations, and pass it through the frozen `vision_model` to get the feature vector. Store these features in a dictionary mapping image paths to feature tensors. This avoids redundant computation during training batch creation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1.1: Extracting image features for sample data...\n",
      "Found 3 unique images to process.\n",
      "  Extracted features for 'green_circle.png', shape: torch.Size([512])\n",
      "  Extracted features for 'blue_square.png', shape: torch.Size([512])\n",
      "  Extracted features for 'red_square.png', shape: torch.Size([512])\n",
      "Finished extracting image features for all unique sample images.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 1.1: Extracting image features for sample data...\")\n",
    "extracted_image_features = {} # Dict to store {image_path: feature_tensor}\n",
    "\n",
    "# --- Loop Through Unique Image Paths ---\n",
    "unique_image_paths = set(d[\"image_path\"] for d in sample_training_data)\n",
    "print(f\"Found {len(unique_image_paths)} unique images to process.\")\n",
    "\n",
    "for img_path in unique_image_paths:\n",
    "    # --- Load Image ---\n",
    "    # Theory: Open the image file using Pillow.\n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB') # Ensure image is RGB\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image file not found at {img_path}. Skipping.\")\n",
    "        continue\n",
    "\n",
    "    # --- Apply Transformations ---\n",
    "    # Theory: Apply the predefined preprocessing pipeline (resize, crop, tensor, normalize).\n",
    "    # Unsqueeze(0) adds a batch dimension (B=1) as expected by the vision model.\n",
    "    img_tensor = image_transforms(img).unsqueeze(0).to(device) # Shape: (1, 3, 224, 224)\n",
    "\n",
    "    # --- Extract Features ---\n",
    "    # Theory: Pass the preprocessed image tensor through the vision model.\n",
    "    # Use torch.no_grad() as we are not training the vision model here.\n",
    "    with torch.no_grad():\n",
    "        feature_vector = vision_model(img_tensor) # Shape: (1, vision_feature_dim)\n",
    "\n",
    "    # --- Store Features ---\n",
    "    extracted_image_features[img_path] = feature_vector.squeeze(0) # Remove batch dim, store (feature_dim,)\n",
    "    print(f\"  Extracted features for '{os.path.basename(img_path)}', shape: {extracted_image_features[img_path].shape}\")\n",
    "\n",
    "print(\"Finished extracting image features for all unique sample images.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2: Tokenize Prompts and Responses\n",
    "\n",
    "**Theory:** Convert the text prompts and responses from our sample data into sequences of integer IDs using the updated `char_to_int` mapping (which now includes `<IMG>`, `<PAD>`, `<EOS>`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1.2: Tokenizing prompts and responses...\n",
      "Added 3 new characters to vocabulary. New vocab_size: 42\n",
      "Tokenized text for all 6 samples.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 1.2: Tokenizing prompts and responses...\")\n",
    "\n",
    "# First, extend vocabulary with any new characters\n",
    "current_vocab_size = vocab_size  # Start from current vocabulary size\n",
    "all_chars = set()\n",
    "\n",
    "# Collect all unique characters from prompts and responses\n",
    "for sample in sample_training_data:\n",
    "    all_chars.update(sample[\"prompt\"])\n",
    "    # Remove EOS token from response before collecting chars\n",
    "    response_text = sample[\"response\"]\n",
    "    if response_text.endswith(eos_token):\n",
    "        response_text = response_text[:-len(eos_token)]\n",
    "    all_chars.update(response_text)\n",
    "\n",
    "# Add any new characters to vocabulary\n",
    "new_chars_added = 0\n",
    "for char in all_chars:\n",
    "    if char not in char_to_int:\n",
    "        char_to_int[char] = current_vocab_size\n",
    "        int_to_char[current_vocab_size] = char\n",
    "        current_vocab_size += 1\n",
    "        new_chars_added += 1\n",
    "\n",
    "vocab_size = current_vocab_size  # Update vocab size\n",
    "print(f\"Added {new_chars_added} new characters to vocabulary. New vocab_size: {vocab_size}\")\n",
    "\n",
    "# Now tokenize with the extended vocabulary\n",
    "tokenized_samples = []\n",
    "for sample in sample_training_data:\n",
    "    # --- Tokenize Prompt ---\n",
    "    prompt_ids = [char_to_int[ch] for ch in sample[\"prompt\"]]\n",
    "\n",
    "    # --- Tokenize Response ---\n",
    "    response_text = sample[\"response\"]\n",
    "    if response_text.endswith(eos_token):\n",
    "        response_text_without_eos = response_text[:-len(eos_token)]\n",
    "        response_ids = [char_to_int[ch] for ch in response_text_without_eos] + [char_to_int[eos_token]]\n",
    "    else:\n",
    "        response_ids = [char_to_int[ch] for ch in response_text]\n",
    "\n",
    "    tokenized_samples.append({\n",
    "        \"image_path\": sample[\"image_path\"],\n",
    "        \"prompt_ids\": prompt_ids,\n",
    "        \"response_ids\": response_ids\n",
    "    })\n",
    "\n",
    "print(f\"Tokenized text for all {len(tokenized_samples)} samples.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.3: Create Padded Input/Target Sequences and Masks\n",
    "\n",
    "**Theory:** Combine the image representation, tokenized prompt, and tokenized response into a single input sequence for the Transformer. Create the corresponding target sequence (shifted response) and an attention mask.\n",
    "1.  **Input Sequence (`x`):** `[<IMG>_id] * num_img_tokens + prompt_ids + response_ids` (excluding the last token of response as it won't be predicted). Pad this sequence with `pad_token_id` up to `block_size`.\n",
    "2.  **Target Sequence (`y`):** The target is to predict the *next* token. So, the target sequence corresponds to `prompt_ids + response_ids`, shifted one position. We'll set targets for the `<IMG>` and `prompt` parts to be ignored during loss calculation (using `ignore_index` which is often -100, but CrossEntropyLoss handles it via `ignore_index=pad_token_id` if we structure it carefully, or we manually mask the loss later). Pad with `pad_token_id`.\n",
    "3.  **Attention Mask:** A mask indicating which positions the Transformer should attend to. It should prevent attention to `<PAD>` tokens. It combines the causal mask (for self-attention) and the padding mask. For simplicity here, we'll primarily create a padding mask and rely on the pre-computed `causal_mask` during the attention calculation. The padding mask is `1` for real tokens and `0` for pad tokens."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1.3: Creating padded input/target sequences and masks...\n",
      "Created 6 padded sequences with targets and masks.\n",
      "  Input IDs shape: torch.Size([6, 64])\n",
      "  Target IDs shape: torch.Size([6, 65])\n",
      "  Attention Mask shape: torch.Size([6, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 1.3: Creating padded input/target sequences and masks...\")\n",
    "\n",
    "prepared_sequences = []\n",
    "ignore_index = -100 # Common ignore index for CrossEntropyLoss\n",
    "\n",
    "for sample in tokenized_samples:\n",
    "    # --- Construct Input Sequence IDs ---\n",
    "    img_ids = [char_to_int[img_token]] * num_img_tokens\n",
    "    input_ids_no_pad = img_ids + sample[\"prompt_ids\"] + sample[\"response_ids\"][:-1] # Input predicts response\n",
    "\n",
    "    # --- Construct Target Sequence IDs ---\n",
    "    # Targets are shifted input: predict next token.\n",
    "    # We only care about predicting the response tokens.\n",
    "    # Set targets for image and prompt tokens to 'ignore_index'.\n",
    "    target_ids_no_pad = ([ignore_index] * len(img_ids)) + \\\n",
    "                         ([ignore_index] * len(sample[\"prompt_ids\"])) + \\\n",
    "                         sample[\"response_ids\"]\n",
    "\n",
    "    # --- Padding ---\n",
    "    current_len = len(input_ids_no_pad)\n",
    "    pad_len = block_size - current_len\n",
    "\n",
    "    if pad_len < 0:\n",
    "        print(f\"Warning: Sample sequence length ({current_len}) exceeds block_size ({block_size}). Truncating.\")\n",
    "        input_ids = input_ids_no_pad[:block_size]\n",
    "        target_ids = target_ids_no_pad[:block_size]\n",
    "        pad_len = 0 # No padding needed after truncation\n",
    "        current_len = block_size\n",
    "    else:\n",
    "        input_ids = input_ids_no_pad + ([pad_token_id] * pad_len)\n",
    "        target_ids = target_ids_no_pad + ([ignore_index] * pad_len) # Pad targets with ignore_index\n",
    "\n",
    "    # --- Create Attention Mask (Padding Mask) ---\n",
    "    # 1 for real tokens, 0 for padding tokens.\n",
    "    attention_mask = ([1] * current_len) + ([0] * pad_len)\n",
    "\n",
    "    # --- Store ---\n",
    "    prepared_sequences.append({\n",
    "        \"image_path\": sample[\"image_path\"],\n",
    "        \"input_ids\": torch.tensor(input_ids, dtype=torch.long),\n",
    "        \"target_ids\": torch.tensor(target_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long) # Or float for some implementations\n",
    "    })\n",
    "\n",
    "    # print(f\"  Prepared Sample (Input IDs): {prepared_sequences[-1]['input_ids'].tolist()}\")\n",
    "    # print(f\"  Prepared Sample (Target IDs): {prepared_sequences[-1]['target_ids'].tolist()}\")\n",
    "    # print(f\"  Prepared Sample (Attn Mask): {prepared_sequences[-1]['attention_mask'].tolist()}\")\n",
    "\n",
    "\n",
    "# --- Stack into Tensors ---\n",
    "# Theory: Group the processed sequences into tensors for easier batching.\n",
    "all_input_ids = torch.stack([s['input_ids'] for s in prepared_sequences])\n",
    "all_target_ids = torch.stack([s['target_ids'] for s in prepared_sequences])\n",
    "all_attention_masks = torch.stack([s['attention_mask'] for s in prepared_sequences])\n",
    "# Keep image paths associated for retrieving features during batching\n",
    "all_image_paths = [s['image_path'] for s in prepared_sequences]\n",
    "\n",
    "num_sequences_available = all_input_ids.shape[0]\n",
    "print(f\"Created {num_sequences_available} padded sequences with targets and masks.\")\n",
    "print(f\"  Input IDs shape: {all_input_ids.shape}\") # (num_samples, block_size)\n",
    "print(f\"  Target IDs shape: {all_target_ids.shape}\") # (num_samples, block_size)\n",
    "print(f\"  Attention Mask shape: {all_attention_masks.shape}\") # (num_samples, block_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "All sequences adjusted to maximum length of 64\n"
     ]
    }
   ],
   "source": [
    "# Define maximum sequence length for consistency\n",
    "max_seq_len = 64  # This should match what your model expects\n",
    "\n",
    "# Ensure all sequences are properly sized\n",
    "for i in range(len(tokenized_samples)):\n",
    "    # Ensure response_ids don't exceed max length\n",
    "    if len(tokenized_samples[i][\"response_ids\"]) > max_seq_len:\n",
    "        tokenized_samples[i][\"response_ids\"] = tokenized_samples[i][\"response_ids\"][:max_seq_len]\n",
    "    \n",
    "    # Also check prompt_ids if needed\n",
    "    if len(tokenized_samples[i][\"prompt_ids\"]) > max_seq_len:\n",
    "        tokenized_samples[i][\"prompt_ids\"] = tokenized_samples[i][\"prompt_ids\"][:max_seq_len]\n",
    "\n",
    "print(f\"All sequences adjusted to maximum length of {max_seq_len}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.4: Batching Strategy (Random Sampling)\n",
    "\n",
    "**Theory:** Similar to the text-only model, we'll use simple random sampling. In each training step, we randomly select `batch_size` indices and retrieve the corresponding input IDs, target IDs, attention masks, and image features (using the associated image path)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1.4: Preparing for batching...\n",
      "Data ready for training. Will sample batches of size 4 randomly.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 1.4: Preparing for batching...\")\n",
    "\n",
    "# Check if batch size is feasible\n",
    "if num_sequences_available < batch_size:\n",
    "    print(f\"Warning: Number of sequences ({num_sequences_available}) is less than batch size ({batch_size}). Adjusting batch size.\")\n",
    "    batch_size = num_sequences_available\n",
    "\n",
    "print(f\"Data ready for training. Will sample batches of size {batch_size} randomly.\")\n",
    "# In the training loop, we will use random indices to get:\n",
    "# xb_ids = all_input_ids[indices]\n",
    "# yb_ids = all_target_ids[indices]\n",
    "# batch_masks = all_attention_masks[indices]\n",
    "# batch_img_paths = [all_image_paths[i] for i in indices.tolist()]\n",
    "# batch_img_features = torch.stack([extracted_image_features[p] for p in batch_img_paths])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Model Adaptation and Initialization\n",
    "\n",
    "**Goal:** Re-initialize the parts of the model that changed (embedding table, output layer due to vocab size) and initialize the new vision projection layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1: Re-initialize Embedding and Output Layers\n",
    "\n",
    "**Theory:** Since we added special tokens (`<IMG>`, `<PAD>`, `<EOS>`), the `vocab_size` has changed. The `token_embedding_table` and the `output_linear_layer` need to be resized to accommodate this. We load the weights from the saved state for the *original* vocabulary part and initialize the new rows (for the special tokens) randomly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2.1: Re-initializing Embedding and Output Layers for new vocab size...\n",
      "  Re-initialized Token Embedding Table, shape: torch.Size([42, 64])\n",
      "  Re-initialized Output Linear Layer, weight shape: torch.Size([42, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 2.1: Re-initializing Embedding and Output Layers for new vocab size...\")\n",
    "\n",
    "# --- Token Embedding Table ---\n",
    "new_token_embedding_table = nn.Embedding(vocab_size, d_model).to(device)\n",
    "# Load weights for the original vocabulary part\n",
    "original_weights = loaded_state_dict['token_embedding_table']['weight'][:loaded_vocab_size, :]\n",
    "with torch.no_grad():\n",
    "    new_token_embedding_table.weight[:loaded_vocab_size, :] = original_weights\n",
    "    # New tokens (<IMG>, <PAD>, <EOS>) are randomly initialized by default, which is fine.\n",
    "token_embedding_table = new_token_embedding_table # Replace the old variable\n",
    "print(f\"  Re-initialized Token Embedding Table, shape: {token_embedding_table.weight.shape}\")\n",
    "\n",
    "# --- Output Linear Layer ---\n",
    "new_output_linear_layer = nn.Linear(d_model, vocab_size).to(device)\n",
    "# Load weights and biases for the original vocabulary part\n",
    "original_out_weight = loaded_state_dict['output_linear_layer']['weight'][:loaded_vocab_size, :]\n",
    "original_out_bias = loaded_state_dict['output_linear_layer']['bias'][:loaded_vocab_size]\n",
    "with torch.no_grad():\n",
    "    new_output_linear_layer.weight[:loaded_vocab_size, :] = original_out_weight\n",
    "    new_output_linear_layer.bias[:loaded_vocab_size] = original_out_bias\n",
    "    # Weights/biases for new tokens are randomly initialized.\n",
    "output_linear_layer = new_output_linear_layer # Replace the old variable\n",
    "print(f\"  Re-initialized Output Linear Layer, weight shape: {output_linear_layer.weight.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2: Initialize Vision Projection Layer\n",
    "\n",
    "**Theory:** Create and initialize the new linear layer that projects the extracted image features (dimension `vision_feature_dim`) to the Transformer's hidden dimension (`d_model`). This layer is crucial for making the image information compatible with the text embeddings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2.2: Initializing Vision Projection Layer...\n",
      "  Initialized Vision Projection Layer: 512 -> 64. Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 2.2: Initializing Vision Projection Layer...\")\n",
    "\n",
    "vision_projection_layer = nn.Linear(vision_feature_dim, d_model).to(device)\n",
    "\n",
    "print(f\"  Initialized Vision Projection Layer: {vision_feature_dim} -> {d_model}. Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.3: Load Existing Transformer Block Layers\n",
    "\n",
    "**Theory:** We need to reload the parameters for the core Transformer blocks (LayerNorms, QKV/Output Linears for MHA, FFN Linears) from the loaded state dictionary. We place them back into lists, just like in `transformer2.ipynb`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2.3: Loading parameters for existing Transformer Blocks...\n",
      "  Loaded components for Layer 1/3.\n",
      "  Loaded components for Layer 2/3.\n",
      "  Loaded components for Layer 3/3.\n",
      "  Loaded Final LayerNorm.\n",
      "Warning: Loaded positional encoding size (32) != new block_size (64). Recomputing.\n",
      "  Recomputed Positional Encoding matrix, shape: torch.Size([1, 64, 64])\n",
      "Finished loading existing model components.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 2.3: Loading parameters for existing Transformer Blocks...\")\n",
    "\n",
    "# Lists to store layers for each Transformer block\n",
    "layer_norms_1 = []\n",
    "layer_norms_2 = []\n",
    "mha_qkv_linears = []\n",
    "mha_output_linears = []\n",
    "ffn_linear_1 = []\n",
    "ffn_linear_2 = []\n",
    "\n",
    "# Load components for each layer from the state dict\n",
    "for i in range(n_layers):\n",
    "    # LayerNorm 1\n",
    "    ln1 = nn.LayerNorm(d_model).to(device)\n",
    "    ln1.load_state_dict(loaded_state_dict['layer_norms_1'][i])\n",
    "    layer_norms_1.append(ln1)\n",
    "\n",
    "    # MHA QKV Linear\n",
    "    qkv_linear = nn.Linear(d_model, 3 * d_model, bias=False).to(device)\n",
    "    qkv_linear.load_state_dict(loaded_state_dict['mha_qkv_linears'][i])\n",
    "    mha_qkv_linears.append(qkv_linear)\n",
    "\n",
    "    # MHA Output Linear\n",
    "    output_linear_mha = nn.Linear(d_model, d_model).to(device)\n",
    "    output_linear_mha.load_state_dict(loaded_state_dict['mha_output_linears'][i])\n",
    "    mha_output_linears.append(output_linear_mha) # Renamed to avoid conflict\n",
    "\n",
    "    # LayerNorm 2\n",
    "    ln2 = nn.LayerNorm(d_model).to(device)\n",
    "    ln2.load_state_dict(loaded_state_dict['layer_norms_2'][i])\n",
    "    layer_norms_2.append(ln2)\n",
    "\n",
    "    # FFN Linear 1\n",
    "    lin1 = nn.Linear(d_model, d_ff).to(device)\n",
    "    lin1.load_state_dict(loaded_state_dict['ffn_linear_1'][i])\n",
    "    ffn_linear_1.append(lin1)\n",
    "\n",
    "    # FFN Linear 2\n",
    "    lin2 = nn.Linear(d_ff, d_model).to(device)\n",
    "    lin2.load_state_dict(loaded_state_dict['ffn_linear_2'][i])\n",
    "    ffn_linear_2.append(lin2)\n",
    "\n",
    "    print(f\"  Loaded components for Layer {i+1}/{n_layers}.\")\n",
    "\n",
    "# Load Final LayerNorm\n",
    "final_layer_norm = nn.LayerNorm(d_model).to(device)\n",
    "final_layer_norm.load_state_dict(loaded_state_dict['final_layer_norm'])\n",
    "print(\"  Loaded Final LayerNorm.\")\n",
    "\n",
    "# Load Positional Encoding (not a parameter, but needed)\n",
    "positional_encoding = loaded_state_dict['positional_encoding'].to(device)\n",
    "# Adjust positional encoding if block_size changed significantly\n",
    "if positional_encoding.shape[1] != block_size:\n",
    "     print(f\"Warning: Loaded positional encoding size ({positional_encoding.shape[1]}) != new block_size ({block_size}). Recomputing.\")\n",
    "     # Recompute PE for the new block_size (copy code from Step 0.5 of transformer2.ipynb if needed)\n",
    "     # Or simply slice/pad the existing one if change is small (less accurate)\n",
    "     # For simplicity, we'll just recreate it using the code from transformer2\n",
    "     new_pe = torch.zeros(block_size, d_model, device=device)\n",
    "     position = torch.arange(0, block_size, dtype=torch.float, device=device).unsqueeze(1)\n",
    "     div_term_indices = torch.arange(0, d_model, 2, dtype=torch.float, device=device)\n",
    "     div_term = torch.exp(div_term_indices * (-math.log(10000.0) / d_model))\n",
    "     new_pe[:, 0::2] = torch.sin(position * div_term)\n",
    "     new_pe[:, 1::2] = torch.cos(position * div_term)\n",
    "     positional_encoding = new_pe.unsqueeze(0) # Add batch dim\n",
    "     print(f\"  Recomputed Positional Encoding matrix, shape: {positional_encoding.shape}\")\n",
    "\n",
    "\n",
    "print(\"Finished loading existing model components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.4: Define Optimizer and Loss Function\n",
    "\n",
    "**Theory:** Gather *all* trainable parameters, including the newly initialized vision projection layer and the resized embedding/output layers, into a list. Define the AdamW optimizer to manage these parameters. Define the Cross-Entropy loss function, making sure to set `ignore_index=ignore_index` (which we set to -100 earlier) so that padding tokens and non-target tokens (like prompt tokens) do not contribute to the loss calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2.4: Defining Optimizer and Loss Function...\n",
      "  Optimizer defined: AdamW with lr=0.0003\n",
      "  Managing 40 parameter groups/tensors.\n",
      "  Loss function defined: CrossEntropyLoss (ignore_index=-100)\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 2.4: Defining Optimizer and Loss Function...\")\n",
    "\n",
    "# --- Gather All Trainable Parameters ---\n",
    "# Theory: Collect parameters from all components that need gradient updates.\n",
    "# We are fine-tuning the text parts and training the new vision projection.\n",
    "all_trainable_parameters = list(token_embedding_table.parameters())\n",
    "all_trainable_parameters.extend(list(vision_projection_layer.parameters())) # Add new layer\n",
    "for i in range(n_layers):\n",
    "    all_trainable_parameters.extend(list(layer_norms_1[i].parameters()))\n",
    "    all_trainable_parameters.extend(list(mha_qkv_linears[i].parameters()))\n",
    "    all_trainable_parameters.extend(list(mha_output_linears[i].parameters())) # Use correct name\n",
    "    all_trainable_parameters.extend(list(layer_norms_2[i].parameters()))\n",
    "    all_trainable_parameters.extend(list(ffn_linear_1[i].parameters()))\n",
    "    all_trainable_parameters.extend(list(ffn_linear_2[i].parameters()))\n",
    "all_trainable_parameters.extend(list(final_layer_norm.parameters()))\n",
    "all_trainable_parameters.extend(list(output_linear_layer.parameters()))\n",
    "\n",
    "# --- Define Optimizer ---\n",
    "optimizer = optim.AdamW(all_trainable_parameters, lr=learning_rate)\n",
    "print(f\"  Optimizer defined: AdamW with lr={learning_rate}\")\n",
    "print(f\"  Managing {len(all_trainable_parameters)} parameter groups/tensors.\")\n",
    "\n",
    "# --- Define Loss Function ---\n",
    "# Theory: Use CrossEntropyLoss with ignore_index set to ignore padding AND prompt tokens.\n",
    "criterion = nn.CrossEntropyLoss(ignore_index=ignore_index)\n",
    "print(f\"  Loss function defined: CrossEntropyLoss (ignore_index={ignore_index})\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Multi-Modal Training Loop (Inline)\n",
    "\n",
    "**Goal:** Fine-tune the model on the prepared multi-modal data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: The Training Loop Structure\n",
    "\n",
    "**Theory:** Iterate for the specified number of `epochs`. In each epoch:\n",
    "1.  Select a random batch of data (indices).\n",
    "2.  Retrieve corresponding image features, input IDs, target IDs, and attention masks.\n",
    "3.  Perform the forward pass: project image features, get text embeddings, combine them, pass through Transformer blocks, get final logits.\n",
    "4.  Calculate the loss using the criterion (which ignores padding/prompt targets).\n",
    "5.  Perform backpropagation and update model weights using the optimizer.\n",
    "6.  Log the loss periodically."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3.1: Starting Multi-Modal Training Loop...\n",
      "  Epoch 1/2000, Loss: 9.1308\n",
      "  Epoch 501/2000, Loss: 0.0025\n",
      "  Epoch 1001/2000, Loss: 0.0013\n",
      "  Epoch 1501/2000, Loss: 0.0005\n",
      "  Epoch 2000/2000, Loss: 0.0004\n",
      "--- Multi-Modal Training Loop Completed ---\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nStep 3.1: Starting Multi-Modal Training Loop...\")\n",
    "\n",
    "# List to track losses\n",
    "losses = []\n",
    "\n",
    "# --- Set Layers to Training Mode ---\n",
    "# Theory: Ensure layers like LayerNorm are in training mode. Vision model remains in eval.\n",
    "token_embedding_table.train()\n",
    "vision_projection_layer.train()\n",
    "for i in range(n_layers):\n",
    "    layer_norms_1[i].train()\n",
    "    mha_qkv_linears[i].train()\n",
    "    mha_output_linears[i].train() # Use correct name\n",
    "    layer_norms_2[i].train()\n",
    "    ffn_linear_1[i].train()\n",
    "    ffn_linear_2[i].train()\n",
    "final_layer_norm.train()\n",
    "output_linear_layer.train()\n",
    "# vision_model remains in eval() mode\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # --- 1. Batch Selection ---\n",
    "    indices = torch.randint(0, num_sequences_available, (batch_size,))\n",
    "    # Retrieve data for the batch\n",
    "    xb_ids = all_input_ids[indices].to(device)          # (B, T)\n",
    "    yb_ids = all_target_ids[indices].to(device)          # (B, T)\n",
    "    batch_masks = all_attention_masks[indices].to(device) # (B, T) - Basic padding mask\n",
    "    batch_img_paths = [all_image_paths[i] for i in indices.tolist()]\n",
    "    # Retrieve pre-extracted features and stack them into a batch\n",
    "    try:\n",
    "        batch_img_features = torch.stack([extracted_image_features[p] for p in batch_img_paths]).to(device) # (B, vision_feature_dim)\n",
    "    except KeyError as e:\n",
    "        print(f\"Error: Missing extracted feature for image path {e}. Ensure Step 1.1 completed correctly. Skipping epoch.\")\n",
    "        continue\n",
    "\n",
    "\n",
    "    # --- 2. Forward Pass (Inline) ---\n",
    "    B, T = xb_ids.shape # T is block_size\n",
    "    C = d_model\n",
    "\n",
    "    # --- Project Image Features ---\n",
    "    # Theory: Map image features to the Transformer's dimension (d_model).\n",
    "    projected_img_features = vision_projection_layer(batch_img_features) # (B, C)\n",
    "    # Unsqueeze to add the sequence dimension for concatenation/addition: (B, 1, C)\n",
    "    projected_img_features = projected_img_features.unsqueeze(1)\n",
    "    # Replicate if num_img_tokens > 1 (not needed here as num_img_tokens is 1)\n",
    "    # if num_img_tokens > 1:\n",
    "    #     projected_img_features = projected_img_features.repeat(1, num_img_tokens, 1) # (B, num_img_tokens, C)\n",
    "\n",
    "\n",
    "    # --- Get Text Embeddings ---\n",
    "    # Theory: Get embeddings for the entire input ID sequence (including <IMG> placeholders).\n",
    "    text_token_embeddings = token_embedding_table(xb_ids) # (B, T, C)\n",
    "\n",
    "    # --- Combine Modalities ---\n",
    "    # Theory: Replace the embedding of the <IMG> token(s) with the projected image features.\n",
    "    # Since num_img_tokens is 1, we replace the embedding at index 0.\n",
    "    combined_embeddings = text_token_embeddings.clone() # Avoid modifying original tensor inplace\n",
    "    combined_embeddings[:, 0:num_img_tokens, :] = projected_img_features # Simple replacement/injection\n",
    "\n",
    "    # --- Add Positional Encoding ---\n",
    "    # Theory: Add positional information to the combined sequence. Slice PE to match T.\n",
    "    pos_enc_slice = positional_encoding[:, :T, :] # (1, T, C)\n",
    "    x = combined_embeddings + pos_enc_slice # (B, T, C)\n",
    "\n",
    "    # --- Transformer Blocks ---\n",
    "    # This part is identical to the text-only model's forward pass,\n",
    "    # but now 'x' contains fused image+text info.\n",
    "    # We also need to incorporate the padding mask into the attention mechanism.\n",
    "    # Create the combined attention mask: causal + padding\n",
    "    # Expand padding mask for multi-head: (B, T) -> (B, 1, 1, T)\n",
    "    padding_mask_expanded = batch_masks.unsqueeze(1).unsqueeze(2) # (B, 1, 1, T)\n",
    "    # Combined mask: causal AND padding. Result is 0 where attention is NOT allowed.\n",
    "    # Causal mask is (1, 1, T, T). Padding mask is (B, 1, 1, T). Broadcasting applies.\n",
    "    combined_attn_mask = causal_mask[:,:,:T,:T] * padding_mask_expanded # (B, 1, T, T)\n",
    "\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        x_input_block = x\n",
    "        # Pre-LN MHA\n",
    "        x_ln1 = layer_norms_1[i](x_input_block)\n",
    "        qkv = mha_qkv_linears[i](x_ln1)\n",
    "        qkv = qkv.view(B, T, n_heads, 3 * d_k).permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * (d_k ** -0.5) # (B, n_heads, T, T)\n",
    "\n",
    "        # Apply Combined Mask (Causal + Padding)\n",
    "        # We need to expand the mask to match the number of heads, or let broadcasting handle it.\n",
    "        # Pytorch masked_fill needs mask shape broadcastable to scores shape.\n",
    "        # combined_attn_mask shape is (B, 1, T, T), attn_scores is (B, n_heads, T, T). Broadcasting works.\n",
    "        attn_scores_masked = attn_scores.masked_fill(combined_attn_mask == 0, float('-inf'))\n",
    "\n",
    "        attention_weights = F.softmax(attn_scores_masked, dim=-1) # (B, n_heads, T, T)\n",
    "        # Handle potential NaNs if a row in softmax is all -inf (e.g., all padding)\n",
    "        attention_weights = torch.nan_to_num(attention_weights)\n",
    "\n",
    "        attn_output = attention_weights @ v # (B, n_heads, T, d_k)\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(B, T, C)\n",
    "        mha_result = mha_output_linears[i](attn_output) # Use correct name\n",
    "        x = x_input_block + mha_result # Residual 1\n",
    "\n",
    "        # Pre-LN FFN\n",
    "        x_input_ffn = x\n",
    "        x_ln2 = layer_norms_2[i](x_input_ffn)\n",
    "        ffn_hidden = ffn_linear_1[i](x_ln2)\n",
    "        ffn_activated = F.relu(ffn_hidden)\n",
    "        ffn_output = ffn_linear_2[i](ffn_activated)\n",
    "        x = x_input_ffn + ffn_output # Residual 2\n",
    "\n",
    "    # --- Final Layers ---\n",
    "    final_norm_output = final_layer_norm(x) # (B, T, C)\n",
    "    logits = output_linear_layer(final_norm_output) # (B, T, vocab_size)\n",
    "\n",
    "    # --- 3. Calculate Loss ---\n",
    "    # Theory: Calculate CrossEntropyLoss between predicted logits and target IDs.\n",
    "    # The loss function automatically ignores targets with value ignore_index (-100).\n",
    "    # Reshape logits and targets for the loss function.\n",
    "    # In Step 3.1: Multi-Modal Training Loop\n",
    "\n",
    "    B_loss, T_loss, V_loss = logits.shape\n",
    "    # print(f\"Shapes: logits={logits.shape}, targets={yb_ids.shape}\")\n",
    "\n",
    "    # Ensure targets match the sequence length dimension of logits\n",
    "    if yb_ids.size(1) != T_loss:\n",
    "\n",
    "        ##### Debugging: Check if the target sequence is longer than the logits sequence\n",
    "        # print(f\"Adjusting target sequence length from {yb_ids.size(1)} to {T_loss}\")\n",
    "        if yb_ids.size(1) > T_loss:\n",
    "            # Truncate if targets are longer\n",
    "            targets_reshaped = yb_ids[:, :T_loss].contiguous().view(-1)\n",
    "        else:\n",
    "            # Pad with ignore_index if targets are shorter (shouldn't happen with the fix above)\n",
    "            padded_targets = torch.full((B_loss, T_loss), ignore_index, device=device)\n",
    "            padded_targets[:, :yb_ids.size(1)] = yb_ids\n",
    "            targets_reshaped = padded_targets.view(-1)\n",
    "    else:\n",
    "        # Shapes match correctly\n",
    "        targets_reshaped = yb_ids.view(-1)\n",
    "\n",
    "    logits_reshaped = logits.view(-1, V_loss)\n",
    "    loss = criterion(logits_reshaped, targets_reshaped)\n",
    "\n",
    "    # --- 4. Zero Gradients ---\n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # --- 5. Backward Pass ---\n",
    "    # Check if loss is valid before backprop\n",
    "    if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "        loss.backward()\n",
    "         # --- Optional: Gradient Clipping ---\n",
    "        # Theory: Prevents exploding gradients, common in Transformers.\n",
    "        # torch.nn.utils.clip_grad_norm_(all_trainable_parameters, max_norm=1.0)\n",
    "\n",
    "        # --- 6. Update Parameters ---\n",
    "        optimizer.step()\n",
    "    else:\n",
    "        print(f\"Warning: Invalid loss detected (NaN or Inf) at epoch {epoch+1}. Skipping optimizer step.\")\n",
    "        loss = None # Set loss to None if invalid\n",
    "\n",
    "    # --- Logging ---\n",
    "    if loss is not None:\n",
    "        current_loss = loss.item()\n",
    "        losses.append(current_loss)\n",
    "        if epoch % eval_interval == 0 or epoch == epochs - 1:\n",
    "            print(f\"  Epoch {epoch+1}/{epochs}, Loss: {current_loss:.4f}\")\n",
    "    elif epoch % eval_interval == 0 or epoch == epochs - 1:\n",
    "         print(f\"  Epoch {epoch+1}/{epochs}, Loss: Invalid (NaN/Inf)\")\n",
    "\n",
    "\n",
    "print(\"--- Multi-Modal Training Loop Completed ---\\\n",
    "\")\n",
    "\n",
    "# Optional: Plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(losses)\n",
    "plt.title(\"Training Loss Over Epochs\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Multi-Modal Generation (Inline)\n",
    "\n",
    "**Goal:** Use the fine-tuned model to generate a response given a new image and a text prompt."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Prepare Input Image and Prompt\n",
    "\n",
    "**Theory:** Select an image (e.g., one from the sample set or a new one) and a text prompt. Preprocess the image, extract its features using the `vision_model`, and project them using the trained `vision_projection_layer`. Tokenize the prompt and prepend the `<IMG>` token ID. This forms the initial context for generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4.1: Preparing input image and prompt for generation...\n",
      "  Processed image: 'green_circle.png'\n",
      "  Projected image features shape: torch.Size([1, 64])\n",
      "  Tokenized prompt: 'Describe this image: ' -> [[36, 41, 18, 30, 16, 29, 22, 15, 18, 1, 31, 21, 22, 30, 1, 22, 25, 14, 20, 18, 8, 1]]\n",
      "  Set max new tokens to generate: 50\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4.1: Preparing input image and prompt for generation...\")\n",
    "\n",
    "# --- Choose Image and Prompt ---\n",
    "# Let's try generating a description for the green circle\n",
    "test_image_path = image_paths[\"green\"]\n",
    "test_prompt_text = \"Describe this image: \"\n",
    "\n",
    "# --- Process Image ---\n",
    "try:\n",
    "    test_img = Image.open(test_image_path).convert('RGB')\n",
    "    test_img_tensor = image_transforms(test_img).unsqueeze(0).to(device)\n",
    "    with torch.no_grad(): # No gradients needed for feature extraction\n",
    "        test_img_features_raw = vision_model(test_img_tensor) # (1, vision_feature_dim)\n",
    "    # Project features using the TRAINED projection layer\n",
    "    vision_projection_layer.eval() # Ensure projection layer is in eval mode\n",
    "    with torch.no_grad():\n",
    "        test_img_features_projected = vision_projection_layer(test_img_features_raw) # (1, d_model)\n",
    "    print(f\"  Processed image: '{os.path.basename(test_image_path)}'\")\n",
    "    print(f\"  Projected image features shape: {test_img_features_projected.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(f\"Error: Test image not found at {test_image_path}. Cannot generate.\")\n",
    "    # Handle error appropriately, maybe exit or skip generation\n",
    "    test_img_features_projected = None # Indicate error\n",
    "\n",
    "# --- Process Prompt ---\n",
    "if test_img_features_projected is not None:\n",
    "    img_id = char_to_int[img_token]\n",
    "    prompt_ids = [char_to_int[ch] for ch in test_prompt_text]\n",
    "    # Initial sequence IDs: [<IMG>, prompt tokens]\n",
    "    initial_context_ids = torch.tensor([[img_id] * num_img_tokens + prompt_ids], dtype=torch.long, device=device) # Shape: (1, 1 + len(prompt))\n",
    "    print(f\"  Tokenized prompt: '{test_prompt_text}' -> {initial_context_ids.tolist()}\")\n",
    "else:\n",
    "     initial_context_ids = None\n",
    "\n",
    "# --- Generation Parameters ---\n",
    "max_new_tokens = 50 # Max characters to generate for the response\n",
    "eos_token_id = char_to_int[eos_token]\n",
    "print(f\"  Set max new tokens to generate: {max_new_tokens}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: The Generation Loop (Autoregressive Decoding)\n",
    "\n",
    "**Theory:** Generate the response character by character.\n",
    "1.  Start with the initial context (image + prompt).\n",
    "2.  In a loop (`torch.no_grad`):\n",
    "    a.  Prepare the current input sequence (making sure it doesn't exceed `block_size`). Get embeddings for token IDs. Replace `<IMG>` embedding with the pre-calculated projected image features. Add positional encoding. Create attention mask (no padding needed initially, only causal).\n",
    "    b.  Pass the prepared input through the Transformer blocks (ensure layers are in `eval` mode).\n",
    "    c.  Get the logits for the *very last* token position in the sequence.\n",
    "    d.  Apply Softmax to get probabilities.\n",
    "    e.  Sample the next token ID based on the probabilities (e.g., using `torch.multinomial` or taking the `argmax` for greedy decoding).\n",
    "    f.  Append the sampled token ID to the current sequence.\n",
    "    g.  If the sampled token is `<EOS>` or `max_new_tokens` are generated, stop.\n",
    "    h.  Repeat."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4.2: Starting generation loop...\n",
      "  Reached max generation length (50). Stopping.\n",
      "--- Generation Loop Finished ---\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 4.2: Starting generation loop...\")\n",
    "\n",
    "# --- Set Model to Evaluation Mode ---\n",
    "token_embedding_table.eval()\n",
    "# vision_projection_layer is already eval\n",
    "for i in range(n_layers):\n",
    "    layer_norms_1[i].eval()\n",
    "    mha_qkv_linears[i].eval()\n",
    "    mha_output_linears[i].eval()\n",
    "    layer_norms_2[i].eval()\n",
    "    ffn_linear_1[i].eval()\n",
    "    ffn_linear_2[i].eval()\n",
    "final_layer_norm.eval()\n",
    "output_linear_layer.eval()\n",
    "\n",
    "# --- Generation ---\n",
    "generated_sequence_ids = initial_context_ids # Start with image + prompt\n",
    "if generated_sequence_ids is not None:\n",
    "    with torch.no_grad():\n",
    "        for _ in range(max_new_tokens):\n",
    "            # --- Prepare Input ---\n",
    "            current_ids_context = generated_sequence_ids[:, -block_size:] # Ensure context fits block_size\n",
    "            B_gen, T_gen = current_ids_context.shape\n",
    "            C_gen = d_model\n",
    "\n",
    "            # Get embeddings for current context IDs\n",
    "            current_token_embeddings = token_embedding_table(current_ids_context) # (B_gen, T_gen, C_gen)\n",
    "\n",
    "            # Inject image features (only needed if <IMG> is within the current context window)\n",
    "            # Find the position of the <IMG> token(s) in the current context\n",
    "            img_token_pos = -1\n",
    "            if img_id in current_ids_context[0].tolist():\n",
    "                 # Simple check assuming only one <IMG> at the start of the original sequence\n",
    "                 if current_ids_context[0, 0] == img_id:\n",
    "                      img_token_pos = 0\n",
    "\n",
    "            gen_combined_embeddings = current_token_embeddings\n",
    "            if img_token_pos != -1: # Check if the image token is present in the window\n",
    "                 # Inject pre-calculated projected features for the image token(s)\n",
    "                 # Assume only one image token at pos 0 for simplicity\n",
    "                 gen_combined_embeddings[:, img_token_pos:(img_token_pos + num_img_tokens), :] = test_img_features_projected # (1, C) broadcasted/sliced\n",
    "\n",
    "\n",
    "            # Add Positional Encoding (sliced for current context length T_gen)\n",
    "            pos_enc_slice_gen = positional_encoding[:, :T_gen, :]\n",
    "            x_gen = gen_combined_embeddings + pos_enc_slice_gen\n",
    "\n",
    "            # --- Transformer Blocks ---\n",
    "            # Create causal mask for current length T_gen\n",
    "            gen_causal_mask = causal_mask[:,:,:T_gen,:T_gen] # (1, 1, T_gen, T_gen)\n",
    "            # No padding mask needed here as we generate one token at a time / handle context length\n",
    "\n",
    "            for i in range(n_layers):\n",
    "                x_input_block_gen = x_gen\n",
    "                # Pre-LN MHA\n",
    "                x_ln1_gen = layer_norms_1[i](x_input_block_gen)\n",
    "                qkv_gen = mha_qkv_linears[i](x_ln1_gen)\n",
    "                qkv_gen = qkv_gen.view(B_gen, T_gen, n_heads, 3 * d_k).permute(0, 2, 1, 3)\n",
    "                q_gen, k_gen, v_gen = qkv_gen.chunk(3, dim=-1)\n",
    "                attn_scores_gen = (q_gen @ k_gen.transpose(-2, -1)) * (d_k ** -0.5)\n",
    "                attn_scores_masked_gen = attn_scores_gen.masked_fill(gen_causal_mask == 0, float('-inf'))\n",
    "                attention_weights_gen = F.softmax(attn_scores_masked_gen, dim=-1)\n",
    "                attention_weights_gen = torch.nan_to_num(attention_weights_gen)\n",
    "                attn_output_gen = attention_weights_gen @ v_gen\n",
    "                attn_output_gen = attn_output_gen.permute(0, 2, 1, 3).contiguous().view(B_gen, T_gen, C_gen)\n",
    "                mha_result_gen = mha_output_linears[i](attn_output_gen)\n",
    "                x_gen = x_input_block_gen + mha_result_gen # Residual 1\n",
    "                # Pre-LN FFN\n",
    "                x_input_ffn_gen = x_gen\n",
    "                x_ln2_gen = layer_norms_2[i](x_input_ffn_gen)\n",
    "                ffn_hidden_gen = ffn_linear_1[i](x_ln2_gen)\n",
    "                ffn_activated_gen = F.relu(ffn_hidden_gen)\n",
    "                ffn_output_gen = ffn_linear_2[i](ffn_activated_gen)\n",
    "                x_gen = x_input_ffn_gen + ffn_output_gen # Residual 2\n",
    "\n",
    "            # --- Final Layers ---\n",
    "            final_norm_output_gen = final_layer_norm(x_gen)\n",
    "            logits_gen = output_linear_layer(final_norm_output_gen) # (B_gen, T_gen, vocab_size)\n",
    "\n",
    "            # --- Get Logits for Last Token ---\n",
    "            logits_last_token = logits_gen[:, -1, :] # (B_gen, vocab_size)\n",
    "\n",
    "            # --- Sample Next Token ---\n",
    "            probs = F.softmax(logits_last_token, dim=-1)\n",
    "            next_token_id = torch.multinomial(probs, num_samples=1) # (B_gen, 1)\n",
    "\n",
    "            # --- Append and Check for EOS ---\n",
    "            generated_sequence_ids = torch.cat((generated_sequence_ids, next_token_id), dim=1)\n",
    "\n",
    "            if next_token_id.item() == eos_token_id:\n",
    "                print(\"  <EOS> token generated. Stopping.\")\n",
    "                break\n",
    "        else: # Loop finished without hitting EOS\n",
    "             print(f\"  Reached max generation length ({max_new_tokens}). Stopping.\")\n",
    "\n",
    "else:\n",
    "    print(\"Generation skipped due to error in preparing input.\")\n",
    "\n",
    "print(\"--- Generation Loop Finished ---\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3: Decode Generated Sequence\n",
    "\n",
    "**Theory:** Convert the final sequence of token IDs (including the initial prompt and the newly generated tokens) back into a human-readable string using the `int_to_char` mapping."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4.3: Decoding generated sequence...\n",
      "\n",
      "--- Final Generated Output ---\n",
      "Image: green_circle.png\n",
      "Prompt: Describe this image: \n",
      "Generated Response: ge:   of    siiiiiiiiiiittttttttttteeeeeeeeeeeeeeeeeee\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 4.3: Decoding generated sequence...\")\n",
    "\n",
    "if generated_sequence_ids is not None:\n",
    "    # Extract the generated IDs (excluding the initial <IMG> token if desired)\n",
    "    final_ids_list = generated_sequence_ids[0].tolist()\n",
    "\n",
    "    # Decode the full sequence\n",
    "    decoded_text = \"\"\n",
    "    for id_val in final_ids_list:\n",
    "        # Avoid trying to decode potential ignore_index if it slipped through\n",
    "        if id_val in int_to_char:\n",
    "            decoded_text += int_to_char[id_val]\n",
    "        else:\n",
    "            decoded_text += f\"[UNK:{id_val}]\" # Handle unknown IDs\n",
    "\n",
    "\n",
    "    print(f\"\\n--- Final Generated Output ---\")\n",
    "    print(f\"Image: {os.path.basename(test_image_path)}\")\n",
    "    # Displaying the prompt and response part\n",
    "    # Find start of response (first token after prompt)\n",
    "    response_start_index = num_img_tokens + len(test_prompt_text)\n",
    "    print(f\"Prompt: {test_prompt_text}\")\n",
    "    print(f\"Generated Response: {decoded_text[response_start_index:]}\")\n",
    "    # print(f\"Full Decoded Sequence: {decoded_text}\") # Optional: print everything\n",
    "else:\n",
    "    print(\"Decoding skipped.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Save the model state (optional)\n",
    "\n",
    "To save our pretrained multi-modal model, you need to create a dictionary with all model components and configurations, then use torch.save(). Here's how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Multi-modal model saved to saved_models\\multimodal_model.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "save_dir = 'saved_models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'multimodal_model.pt')\n",
    "\n",
    "# Create a dictionary with all model components and configurations\n",
    "multimodal_state_dict = {\n",
    "    # Configuration\n",
    "    'config': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'd_model': d_model,\n",
    "        'n_heads': n_heads,\n",
    "        'n_layers': n_layers,\n",
    "        'd_ff': d_ff,\n",
    "        'block_size': block_size,\n",
    "        'num_img_tokens': num_img_tokens,\n",
    "        'vision_feature_dim': vision_feature_dim\n",
    "    },\n",
    "    # Tokenizer\n",
    "    'tokenizer': {\n",
    "        'char_to_int': char_to_int,\n",
    "        'int_to_char': int_to_char\n",
    "    },\n",
    "    # Model weights\n",
    "    'token_embedding_table': token_embedding_table.state_dict(),\n",
    "    'vision_projection_layer': vision_projection_layer.state_dict(),\n",
    "    'positional_encoding': positional_encoding,\n",
    "    'layer_norms_1': [ln.state_dict() for ln in layer_norms_1],\n",
    "    'mha_qkv_linears': [l.state_dict() for l in mha_qkv_linears],\n",
    "    'mha_output_linears': [l.state_dict() for l in mha_output_linears],\n",
    "    'layer_norms_2': [ln.state_dict() for ln in layer_norms_2],\n",
    "    'ffn_linear_1': [l.state_dict() for l in ffn_linear_1],\n",
    "    'ffn_linear_2': [l.state_dict() for l in ffn_linear_2],\n",
    "    'final_layer_norm': final_layer_norm.state_dict(),\n",
    "    'output_linear_layer': output_linear_layer.state_dict()\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "torch.save(multimodal_state_dict, save_path)\n",
    "print(f\"Multi-modal model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here's how to load our saved multi-modal model:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state dictionary from 'saved_models/multimodal_model.pt'.\n",
      "Multi-modal model components loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved model state dictionary\n",
    "model_load_path = 'saved_models/multimodal_model.pt'\n",
    "loaded_state_dict = torch.load(model_load_path, map_location=device)\n",
    "print(f\"Loaded state dictionary from '{model_load_path}'.\")\n",
    "\n",
    "# Extract configuration and tokenizer\n",
    "config = loaded_state_dict['config']\n",
    "vocab_size = config['vocab_size']\n",
    "d_model = config['d_model']\n",
    "n_heads = config['n_heads']\n",
    "n_layers = config['n_layers']\n",
    "d_ff = config['d_ff']\n",
    "block_size = config['block_size']\n",
    "num_img_tokens = config['num_img_tokens']\n",
    "vision_feature_dim = config['vision_feature_dim']\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "char_to_int = loaded_state_dict['tokenizer']['char_to_int']\n",
    "int_to_char = loaded_state_dict['tokenizer']['int_to_char']\n",
    "\n",
    "# Recreate causal mask\n",
    "causal_mask = torch.tril(torch.ones(block_size, block_size, device=device)).view(1, 1, block_size, block_size)\n",
    "\n",
    "# Rebuild model components\n",
    "token_embedding_table = nn.Embedding(vocab_size, d_model).to(device)\n",
    "token_embedding_table.load_state_dict(loaded_state_dict['token_embedding_table'])\n",
    "\n",
    "vision_projection_layer = nn.Linear(vision_feature_dim, d_model).to(device)\n",
    "vision_projection_layer.load_state_dict(loaded_state_dict['vision_projection_layer'])\n",
    "\n",
    "positional_encoding = loaded_state_dict['positional_encoding'].to(device)\n",
    "\n",
    "# Initialize transformer layers\n",
    "layer_norms_1 = []\n",
    "mha_qkv_linears = []\n",
    "mha_output_linears = []\n",
    "layer_norms_2 = []\n",
    "ffn_linear_1 = []\n",
    "ffn_linear_2 = []\n",
    "\n",
    "for i in range(n_layers):\n",
    "    # For each layer, create components and load their state dictionaries\n",
    "    ln1 = nn.LayerNorm(d_model).to(device)\n",
    "    ln1.load_state_dict(loaded_state_dict['layer_norms_1'][i])\n",
    "    layer_norms_1.append(ln1)\n",
    "    \n",
    "    # For the QKV linear layer, we need to match the bias parameter\n",
    "    qkv_dict = loaded_state_dict['mha_qkv_linears'][i]\n",
    "    has_qkv_bias = 'bias' in qkv_dict\n",
    "    qkv = nn.Linear(d_model, 3 * d_model, bias=has_qkv_bias).to(device)\n",
    "    qkv.load_state_dict(qkv_dict)\n",
    "    mha_qkv_linears.append(qkv)\n",
    "    \n",
    "    # Similar approach for other linear layers\n",
    "    out_dict = loaded_state_dict['mha_output_linears'][i]\n",
    "    has_out_bias = 'bias' in out_dict\n",
    "    out = nn.Linear(d_model, d_model, bias=has_out_bias).to(device)\n",
    "    out.load_state_dict(out_dict)\n",
    "    mha_output_linears.append(out)\n",
    "    \n",
    "    ln2 = nn.LayerNorm(d_model).to(device)\n",
    "    ln2.load_state_dict(loaded_state_dict['layer_norms_2'][i])\n",
    "    layer_norms_2.append(ln2)\n",
    "    \n",
    "    ff1_dict = loaded_state_dict['ffn_linear_1'][i]\n",
    "    has_ff1_bias = 'bias' in ff1_dict\n",
    "    ff1 = nn.Linear(d_model, d_ff, bias=has_ff1_bias).to(device)\n",
    "    ff1.load_state_dict(ff1_dict)\n",
    "    ffn_linear_1.append(ff1)\n",
    "    \n",
    "    ff2_dict = loaded_state_dict['ffn_linear_2'][i]\n",
    "    has_ff2_bias = 'bias' in ff2_dict\n",
    "    ff2 = nn.Linear(d_ff, d_model, bias=has_ff2_bias).to(device)\n",
    "    ff2.load_state_dict(ff2_dict)\n",
    "    ffn_linear_2.append(ff2)\n",
    "\n",
    "# Final layer norm and output projection\n",
    "final_layer_norm = nn.LayerNorm(d_model).to(device)\n",
    "final_layer_norm.load_state_dict(loaded_state_dict['final_layer_norm'])\n",
    "\n",
    "output_dict = loaded_state_dict['output_linear_layer']\n",
    "has_output_bias = 'bias' in output_dict\n",
    "output_linear_layer = nn.Linear(d_model, vocab_size, bias=has_output_bias).to(device)\n",
    "output_linear_layer.load_state_dict(output_dict)\n",
    "\n",
    "print(\"Multi-modal model components loaded successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After loading the model, you can use it for inference with the same forward pass logic you used during training. To generate text based on an image and a prompt:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate_with_image(image_path, prompt, max_new_tokens=50):\n",
    "    \"\"\"Generate text response for an image and prompt\"\"\"\n",
    "    # Set everything to evaluation mode\n",
    "    token_embedding_table.eval()\n",
    "    vision_projection_layer.eval()\n",
    "    for i in range(n_layers):\n",
    "        layer_norms_1[i].eval()\n",
    "        mha_qkv_linears[i].eval()\n",
    "        mha_output_linears[i].eval()\n",
    "        layer_norms_2[i].eval()\n",
    "        ffn_linear_1[i].eval()\n",
    "        ffn_linear_2[i].eval()\n",
    "    final_layer_norm.eval()\n",
    "    output_linear_layer.eval()\n",
    "    \n",
    "    # Process image\n",
    "    image = Image.open(image_path).convert('RGB')\n",
    "    img_tensor = image_transforms(image).unsqueeze(0).to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Extract and project image features\n",
    "        img_features_raw = vision_model(img_tensor)\n",
    "        img_features_projected = vision_projection_layer(img_features_raw)\n",
    "        \n",
    "        # Tokenize prompt and prepare initial sequence\n",
    "        img_id = char_to_int[img_token]\n",
    "        prompt_ids = [char_to_int[ch] for ch in prompt]\n",
    "        context_ids = torch.tensor([[img_id] + prompt_ids], dtype=torch.long, device=device)\n",
    "        \n",
    "        # Generation loop\n",
    "        for _ in range(max_new_tokens):\n",
    "            # Use only the last block_size tokens if context gets too long\n",
    "            context_ids = context_ids[:, -block_size:]\n",
    "            \n",
    "            # Generation step (using the same forward logic as in training)\n",
    "            # [Generation logic goes here - follow the same steps as in Step 4.2]\n",
    "            \n",
    "            # Get the next token prediction\n",
    "            # [Logic to get next token]\n",
    "            \n",
    "            # Check for EOS token\n",
    "            # [Logic to check for EOS and break]\n",
    "            \n",
    "        # Format and return results\n",
    "        # [Logic to decode and return the result]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Conclusion\n",
    "\n",
    "This notebook demonstrated an end-to-end, inline implementation of extending a pre-trained text-only Transformer into a basic multi-modal model capable of processing image features alongside text prompts.\n",
    "\n",
    "Key steps included:\n",
    "1.  **Loading:** Reusing the previously trained text Transformer's weights and configuration.\n",
    "2.  **Vocabulary Extension:** Adding special tokens (`<IMG>`, `<PAD>`, `<EOS>`) required for the multi-modal setup.\n",
    "3.  **Vision Integration:** Using a pre-trained ResNet-18 to extract image features and adding a learnable projection layer to map these features into the Transformer's embedding space.\n",
    "4.  **Data Preparation:** Creating sample (Image, Prompt, Response) data, extracting features, tokenizing text, and constructing padded input sequences with appropriate target IDs (using `ignore_index`) and attention masks.\n",
    "5.  **Model Adaptation:** Resizing embedding/output layers for the new vocabulary and initializing the vision projection layer.\n",
    "6.  **Training:** Fine-tuning the model (text components + vision projection) on the multi-modal data using a combined input sequence (image features injected into `<IMG>` token embeddings + text embeddings) and masked cross-entropy loss.\n",
    "7.  **Generation:** Performing autoregressive decoding by providing an image (features) and a prompt, then iteratively predicting the next character token until an `<EOS>` token or maximum length is reached.\n",
    "\n",
    "While this inline approach is verbose and uses simplifications (like basic feature injection, small data, no vision model fine-tuning), it meticulously illustrates the fundamental concepts and steps involved in building a multi-modal Transformer system. Real-world applications would leverage more sophisticated fusion techniques (e.g., cross-attention), larger datasets, and standard programming practices (functions, classes) for better code organization and scalability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-multimodaal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
