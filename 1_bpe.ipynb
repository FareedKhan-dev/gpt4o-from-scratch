{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Byte Pair Encoding (BPE) Implementation\n",
    "\n",
    "### Introduction: The Need for Subword Tokenization and BPE\n",
    "\n",
    "**What is Tokenization?**\n",
    "In Natural Language Processing (NLP), tokenization is the fundamental first step of breaking down raw text into smaller units called tokens. These tokens are the basic building blocks that machine learning models process (e.g., words, punctuation marks).\n",
    "\n",
    "**Limitations of Simple Word Tokenization:**\n",
    "Simply splitting text by spaces or punctuation seems intuitive but faces major challenges:\n",
    "1.  **Vocabulary Explosion:** If every unique word is a token, the vocabulary size can become enormous, especially with large corpora containing many word variations (e.g., 'run', 'runs', 'running', 'ran', 'runner'). This requires significant memory and computational resources.\n",
    "2.  **Out-of-Vocabulary (OOV) Problem:** Models trained on a fixed vocabulary cannot handle words they haven't seen during training (e.g., new proper nouns, technical terms, misspellings, newly coined words). These OOV words are typically mapped to a generic `<UNK>` (unknown) token, losing valuable information.\n",
    "3.  **Morphological Relationships:** Word-level tokenization ignores the relationships between words sharing common roots or affixes (e.g., 'running', 'runner').\n",
    "\n",
    "**Subword Tokenization as a Solution:**\n",
    "Subword tokenization algorithms aim to find a middle ground between character-level (tiny vocabulary, very long sequences) and word-level tokenization. They break words into smaller, meaningful units (subwords or morphemes). \n",
    "*   Common words might remain single tokens (e.g., 'the').\n",
    "*   Less common words are broken down (e.g., 'tokenization' -> 'token', 'ization').\n",
    "*   Rare or unknown words are broken down further, potentially to individual characters (e.g., 'BPEology' -> 'B', 'P', 'E', 'ology' or even smaller pieces depending on training).\n",
    "This approach keeps the vocabulary size manageable while drastically reducing the OOV problem and preserving some morphological information.\n",
    "\n",
    "**Byte Pair Encoding (BPE): The Algorithm**\n",
    "BPE, originally a data compression technique, was adapted for NLP tokenization. Its core idea is simple and elegant: **iteratively merge the most frequent pair of adjacent symbols (characters or previously merged subwords) in the training corpus.**\n",
    "\n",
    "**High-Level BPE Steps:**\n",
    "1.  **Initialization:** Start with a vocabulary containing only individual characters found in the training text. Prepare the text by splitting it into words and representing each word as a sequence of its characters plus a special end-of-word marker (essential for learning word boundaries).\n",
    "2.  **Training (Iterative Merging):** Repeat the following for a predefined number of steps (merges):\n",
    "    a.  Count the frequency of all adjacent pairs of symbols across the entire corpus (considering word frequencies).\n",
    "    b.  Find the symbol pair that occurs most frequently.\n",
    "    c.  Merge this pair into a single new symbol (subword).\n",
    "    d.  Add this new symbol to the vocabulary.\n",
    "    e.  Replace all occurrences of the original pair in the corpus representation with the new symbol.\n",
    "3.  **Tokenization (Applying Learned Rules):** To tokenize new text:\n",
    "    a.  Preprocess it in the same way as the training data (split words, add end-of-word markers).\n",
    "    b.  Apply the learned merge operations *in the exact order they were learned* (highest priority first). Greedily merge pairs within each word until no more learned merges can be applied.\n",
    "\n",
    "**This Notebook's Approach:**\n",
    "We will implement this entire process step-by-step, inline, without using Python functions or classes. Each conceptual step will be broken down into the smallest possible code block, accompanied by detailed theory, as requested."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setup - Libraries and Corpus Definition\n",
    "\n",
    "**Goal:** Prepare the environment by importing necessary tools and defining the text data we will work with."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.1: Import Libraries\n",
    "\n",
    "**Theory:** We need basic Python libraries:\n",
    "*   `re`: The regular expression library, primarily used here for splitting the raw text into initial word/token units.\n",
    "*   `collections`: Specifically, we use `collections.Counter` for efficiently counting the frequencies of words and symbol pairs, which is central to BPE."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Libraries 're' and 'collections' imported.\n"
     ]
    }
   ],
   "source": [
    "# Import necessary standard libraries\n",
    "import re \n",
    "import collections\n",
    "\n",
    "print(\"Libraries 're' and 'collections' imported.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.2: Define the Training Corpus\n",
    "\n",
    "**Theory:** BPE is a data-driven algorithm. It learns merge rules based on the patterns present in a training corpus. We will use a small excerpt from Lewis Carroll's \"Alice's Adventures in Wonderland\" as our training data. A larger, more diverse corpus would result in a more general-purpose tokenizer, but this smaller example allows us to trace the process more easily."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training corpus defined (length: 593 characters).\n"
     ]
    }
   ],
   "source": [
    "# Define the raw text corpus for training the BPE tokenizer\n",
    "corpus_raw = \"\"\"\n",
    "Alice was beginning to get very tired of sitting by her sister on the\n",
    "bank, and of having nothing to do: once or twice she had peeped into the\n",
    "book her sister was reading, but it had no pictures or conversations in\n",
    "it, 'and what is the use of a book,' thought Alice 'without pictures or\n",
    "conversation?'\n",
    "So she was considering in her own mind (as well as she could, for the\n",
    "hot day made her feel very sleepy and stupid), whether the pleasure\n",
    "of making a daisy-chain would be worth the trouble of getting up and\n",
    "picking the daisies, when suddenly a White Rabbit with pink eyes ran\n",
    "close by her.\n",
    "\"\"\"\n",
    "\n",
    "print(f\"Training corpus defined (length: {len(corpus_raw)} characters).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Preprocessing and Initialization\n",
    "\n",
    "**Goal:** Transform the raw text corpus into the initial state required for the BPE training algorithm. This involves standardizing the text, splitting it into manageable units, counting frequencies, representing these units as symbol sequences, and defining the starting vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1: Lowercase the Corpus\n",
    "\n",
    "**Theory:** Converting the entire corpus to lowercase ensures that the same word, regardless of its capitalization (e.g., 'Alice' vs. 'alice'), is treated as the same unit during frequency counting and merging. This simplifies the process and prevents learning separate tokens for capitalized variations unless specifically desired (which would require a different preprocessing strategy)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus converted to lowercase.\n"
     ]
    }
   ],
   "source": [
    "# Convert the raw corpus to lowercase\n",
    "corpus_lower = corpus_raw.lower()\n",
    "\n",
    "# Optional: Display a snippet of the lowercased corpus\n",
    "# print(\"Lowercased corpus snippet:\")\n",
    "# print(corpus_lower[:100]) \n",
    "print(\"Corpus converted to lowercase.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2: Initial Word/Token Splitting\n",
    "\n",
    "**Theory:** We need to split the text into its basic constituents. While BPE operates on subwords, it typically starts by considering word-level units (including punctuation). We use a regular expression `r'\\w+|[^\\s\\w]+'` via `re.findall`:\n",
    "*   `\\w+`: Matches one or more alphanumeric characters (letters, numbers, and underscore). This captures standard words.\n",
    "*   `|`: Acts as an OR operator.\n",
    "*   `[^\\s\\w]+`: Matches one or more characters that are *not* whitespace (`\\s`) and *not* word characters (`\\w`). This captures punctuation marks like commas, periods, colons, quotes, parentheses, etc., as separate tokens.\n",
    "The result is a list of strings, where each string is either a word or a punctuation mark."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus split into 127 initial words/tokens.\n",
      "First 3 initial tokens: ['alice', 'was', 'beginning']\n"
     ]
    }
   ],
   "source": [
    "# Define the regular expression for splitting words and punctuation\n",
    "split_pattern = r'\\w+|[^\\s\\w]+'\n",
    "\n",
    "# Apply the regex to the lowercased corpus to get a list of initial tokens\n",
    "initial_word_list = re.findall(split_pattern, corpus_lower)\n",
    "\n",
    "print(f\"Corpus split into {len(initial_word_list)} initial words/tokens.\")\n",
    "# Optional: Display the first few tokens\n",
    "print(f\"First 3 initial tokens: {initial_word_list[:3]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.3: Calculate Word/Token Frequencies\n",
    "\n",
    "**Theory:** The core principle of BPE is merging the *most frequent* pairs. Therefore, we must know how often each unique initial word/token appears in the corpus. Pairs found within more frequent words will have a higher impact on the merge decisions. `collections.Counter` efficiently creates a dictionary-like object mapping each unique item (word/token) to its count."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated frequencies for 86 unique words/tokens.\n",
      "3 Most frequent tokens:\n",
      "  'the': 7\n",
      "  'of': 5\n",
      "  'her': 5\n"
     ]
    }
   ],
   "source": [
    "# Use collections.Counter to count frequencies of items in initial_word_list\n",
    "word_frequencies = collections.Counter(initial_word_list)\n",
    "\n",
    "print(f\"Calculated frequencies for {len(word_frequencies)} unique words/tokens.\")\n",
    "# Display the 3 most frequent tokens and their counts\n",
    "print(\"3 Most frequent tokens:\")\n",
    "for token, count in word_frequencies.most_common(3):\n",
    "    print(f\"  '{token}': {count}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.4: Initial Corpus Representation (Character Split with `</w>`)\n",
    "\n",
    "**Theory:** BPE training operates on sequences of symbols. We need to transform our list of unique words/tokens into this format. For each unique word/token:\n",
    "1.  Split it into a list of individual characters.\n",
    "2.  Append a special end-of-word symbol (we'll use `</w>`) to this list.\n",
    "This `</w>` marker is critically important:\n",
    "*   **Boundary Detection:** It prevents BPE from merging characters across different words. For example, the 's' at the end of 'apples' should not merge with the 'a' at the beginning of 'and'.\n",
    "*   **Learning Word Endings:** It allows the algorithm to learn common word endings as distinct subword units (e.g., 'ing</w>', 'ed</w>', 's</w>').\n",
    "We store this mapping (original word -> list of symbols) in a dictionary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Created initial corpus representation for 86 unique words/tokens.\n",
      "Representation for 'beginning': ['b', 'e', 'g', 'i', 'n', 'n', 'i', 'n', 'g', '</w>']\n",
      "Representation for '.': ['.', '</w>']\n"
     ]
    }
   ],
   "source": [
    "# Define the special end-of-word symbol\n",
    "end_of_word_symbol = '</w>'\n",
    "\n",
    "# Create a dictionary to hold the initial representation of the corpus\n",
    "# Key: original unique word/token, Value: list of characters + end_of_word_symbol\n",
    "initial_corpus_representation = {}\n",
    "\n",
    "# Iterate through the unique words/tokens identified by the frequency counter\n",
    "for word in word_frequencies:\n",
    "    # Convert the word string into a list of its characters\n",
    "    char_list = list(word)\n",
    "    # Append the end-of-word symbol to the list\n",
    "    char_list.append(end_of_word_symbol)\n",
    "    # Store this list in the dictionary with the original word as the key\n",
    "    initial_corpus_representation[word] = char_list\n",
    "\n",
    "print(f\"Created initial corpus representation for {len(initial_corpus_representation)} unique words/tokens.\")\n",
    "# Optional: Display the representation for a sample word\n",
    "example_word = 'beginning'\n",
    "if example_word in initial_corpus_representation:\n",
    "    print(f\"Representation for '{example_word}': {initial_corpus_representation[example_word]}\")\n",
    "example_punct = '.'\n",
    "if example_punct in initial_corpus_representation:\n",
    "    print(f\"Representation for '{example_punct}': {initial_corpus_representation[example_punct]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.5: Build Initial Character Vocabulary\n",
    "\n",
    "**Theory:** The BPE algorithm starts its vocabulary with the set of all individual symbols present in the initial corpus representation. This includes all unique characters from the original text *plus* the special `</w>` symbol we added. Using a Python `set` automatically handles uniqueness â€“ adding an existing character multiple times has no effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Initial vocabulary created with 31 unique symbols.\n",
      "Initial vocabulary symbols: [\"'\", '(', ')', ',', '-', '.', ':', '</w>', '?', 'a', 'b', 'c', 'd', 'e', 'f', 'g', 'h', 'i', 'k', 'l', 'm', 'n', 'o', 'p', 'r', 's', 't', 'u', 'v', 'w', 'y']\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty set to store the unique initial symbols (vocabulary)\n",
    "initial_vocabulary = set()\n",
    "\n",
    "# Iterate through the character lists stored in the initial corpus representation\n",
    "for word in initial_corpus_representation:\n",
    "    # Get the list of symbols for the current word\n",
    "    symbols_list = initial_corpus_representation[word]\n",
    "    # Update the vocabulary set with the symbols from this list\n",
    "    # The `update` method adds all elements from an iterable (like a list) to the set\n",
    "    initial_vocabulary.update(symbols_list)\n",
    "\n",
    "# Although update should have added '</w>', we can explicitly add it for certainty\n",
    "# initial_vocabulary.add(end_of_word_symbol)\n",
    "\n",
    "print(f\"Initial vocabulary created with {len(initial_vocabulary)} unique symbols.\")\n",
    "# Optional: Display the sorted list of initial vocabulary symbols\n",
    "print(f\"Initial vocabulary symbols: {sorted(list(initial_vocabulary))}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: BPE Training - Iterative Merging\n",
    "\n",
    "**Goal:** This is the core BPE learning phase. We will iteratively find the most frequent adjacent pair of symbols in our current corpus representation and merge them into a new, single symbol (subword). This process builds the subword vocabulary and the ordered list of merge rules."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1: Initialize Training State Variables\n",
    "\n",
    "**Theory:** Before starting the loop, we need:\n",
    "1.  `num_merges`: To define how many merge operations we want to perform. This directly controls the final vocabulary size (initial size + `num_merges`). Choosing this value is a hyperparameter; larger values capture more complex subwords but increase vocabulary size.\n",
    "2.  `learned_merges`: An empty dictionary to store the merge rules as we learn them. The key will be the pair tuple (e.g., `('t', 'h')`), and the value will be the merge priority (an integer, starting from 0, indicating *when* the merge was learned). Lower numbers mean higher priority.\n",
    "3.  `current_corpus_split`: A variable to hold the state of the corpus representation as it gets modified by merges in each iteration. We initialize it as a *copy* of the `initial_corpus_representation` to avoid modifying the original starting point.\n",
    "4.  `current_vocab`: A variable to hold the vocabulary as it grows with new merged symbols. We initialize it as a *copy* of the `initial_vocabulary`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training state initialized. Target number of merges: 75\n",
      "Initial vocabulary size: 31\n"
     ]
    }
   ],
   "source": [
    "# Define the desired number of merge operations\n",
    "# This determines how many new subword tokens will be added to the initial character vocab\n",
    "num_merges = 75 # Let's use 75 merges for this example\n",
    "\n",
    "# Initialize an empty dictionary to store the learned merge rules\n",
    "# Format: { (symbol1, symbol2): merge_priority_index }\n",
    "learned_merges = {}\n",
    "\n",
    "# Create a working copy of the corpus representation to modify during training\n",
    "# Using .copy() ensures we don't alter the original initial_corpus_representation\n",
    "current_corpus_split = initial_corpus_representation.copy()\n",
    "\n",
    "# Create a working copy of the vocabulary to modify during training\n",
    "current_vocab = initial_vocabulary.copy()\n",
    "\n",
    "print(f\"Training state initialized. Target number of merges: {num_merges}\")\n",
    "print(f\"Initial vocabulary size: {len(current_vocab)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2: The Main Training Loop Structure\n",
    "\n",
    "**Theory:** We will now iterate `num_merges` times. Inside this loop, we perform the core BPE steps: count pairs, find the best pair, store the rule, create the new symbol, update the corpus representation, and update the vocabulary."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Starting BPE Training Loop (75 iterations) ---\n",
      "\n",
      "Iteration 1/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 156 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('e', '</w>') with frequency 21\n",
      "  Step 2.6: Storing merge rule (Priority: 0)...\n",
      "  Stored: ('e', '</w>') -> Priority 0\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'e</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'e</w>' to vocabulary. Current size: 32\n",
      "\n",
      "Iteration 2/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 161 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('i', 'n') with frequency 16\n",
      "  Step 2.6: Storing merge rule (Priority: 1)...\n",
      "  Stored: ('i', 'n') -> Priority 1\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'in'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'in' to vocabulary. Current size: 33\n",
      "\n",
      "Iteration 3/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 167 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('e', 'r') with frequency 13\n",
      "  Step 2.6: Storing merge rule (Priority: 2)...\n",
      "  Stored: ('e', 'r') -> Priority 2\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'er'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'er' to vocabulary. Current size: 34\n",
      "\n",
      "Iteration 4/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 169 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('t', 'h') with frequency 13\n",
      "  Step 2.6: Storing merge rule (Priority: 3)...\n",
      "  Stored: ('t', 'h') -> Priority 3\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'th'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'th' to vocabulary. Current size: 35\n",
      "\n",
      "Iteration 5/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 174 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('d', '</w>') with frequency 12\n",
      "  Step 2.6: Storing merge rule (Priority: 4)...\n",
      "  Stored: ('d', '</w>') -> Priority 4\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'd</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'd</w>' to vocabulary. Current size: 36\n",
      "\n",
      "Iteration 6/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 175 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('s', '</w>') with frequency 11\n",
      "  Step 2.6: Storing merge rule (Priority: 5)...\n",
      "  Stored: ('s', '</w>') -> Priority 5\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 's</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 's</w>' to vocabulary. Current size: 37\n",
      "\n",
      "Iteration 7/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 177 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('in', 'g') with frequency 9\n",
      "  Step 2.6: Storing merge rule (Priority: 6)...\n",
      "  Stored: ('in', 'g') -> Priority 6\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'ing'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'ing' to vocabulary. Current size: 38\n",
      "\n",
      "Iteration 8/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 176 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('ing', '</w>') with frequency 9\n",
      "  Step 2.6: Storing merge rule (Priority: 7)...\n",
      "  Stored: ('ing', '</w>') -> Priority 7\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'ing</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'ing</w>' to vocabulary. Current size: 39\n",
      "\n",
      "Iteration 9/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 175 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('t', '</w>') with frequency 9\n",
      "  Step 2.6: Storing merge rule (Priority: 8)...\n",
      "  Stored: ('t', '</w>') -> Priority 8\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 't</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 't</w>' to vocabulary. Current size: 40\n",
      "\n",
      "Iteration 10/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 177 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('y', '</w>') with frequency 8\n",
      "  Step 2.6: Storing merge rule (Priority: 9)...\n",
      "  Stored: ('y', '</w>') -> Priority 9\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'y</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'y</w>' to vocabulary. Current size: 41\n",
      "\n",
      "Iteration 11/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 176 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('er', '</w>') with frequency 8\n",
      "  Step 2.6: Storing merge rule (Priority: 10)...\n",
      "  Stored: ('er', '</w>') -> Priority 10\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'er</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'er</w>' to vocabulary. Current size: 42\n",
      "\n",
      "Iteration 12/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 175 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('o', 'n') with frequency 7\n",
      "  Step 2.6: Storing merge rule (Priority: 11)...\n",
      "  Stored: ('o', 'n') -> Priority 11\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'on'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'on' to vocabulary. Current size: 43\n",
      "\n",
      "Iteration 13/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 176 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('th', 'e</w>') with frequency 7\n",
      "  Step 2.6: Storing merge rule (Priority: 12)...\n",
      "  Stored: ('th', 'e</w>') -> Priority 12\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'the</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'the</w>' to vocabulary. Current size: 44\n",
      "\n",
      "Iteration 14/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 175 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('i', 'c') with frequency 6\n",
      "  Step 2.6: Storing merge rule (Priority: 13)...\n",
      "  Stored: ('i', 'c') -> Priority 13\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'ic'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'ic' to vocabulary. Current size: 45\n",
      "\n",
      "Iteration 15/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 177 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('o', '</w>') with frequency 6\n",
      "  Step 2.6: Storing merge rule (Priority: 14)...\n",
      "  Stored: ('o', '</w>') -> Priority 14\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'o</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'o</w>' to vocabulary. Current size: 46\n",
      "\n",
      "Iteration 16/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 177 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('a', 'n') with frequency 6\n",
      "  Step 2.6: Storing merge rule (Priority: 15)...\n",
      "  Stored: ('a', 'n') -> Priority 15\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'an'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'an' to vocabulary. Current size: 47\n",
      "\n",
      "Iteration 17/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 178 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: (',', '</w>') with frequency 6\n",
      "  Step 2.6: Storing merge rule (Priority: 16)...\n",
      "  Stored: (',', '</w>') -> Priority 16\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: ',</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added ',</w>' to vocabulary. Current size: 48\n",
      "\n",
      "Iteration 18/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 177 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('a', 's</w>') with frequency 5\n",
      "  Step 2.6: Storing merge rule (Priority: 17)...\n",
      "  Stored: ('a', 's</w>') -> Priority 17\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'as</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'as</w>' to vocabulary. Current size: 49\n",
      "\n",
      "Iteration 19/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 176 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('o', 'f') with frequency 5\n",
      "  Step 2.6: Storing merge rule (Priority: 18)...\n",
      "  Stored: ('o', 'f') -> Priority 18\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'of'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'of' to vocabulary. Current size: 50\n",
      "\n",
      "Iteration 20/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 175 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('of', '</w>') with frequency 5\n",
      "  Step 2.6: Storing merge rule (Priority: 19)...\n",
      "  Stored: ('of', '</w>') -> Priority 19\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'of</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'of</w>' to vocabulary. Current size: 51\n",
      "\n",
      "Iteration 21/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 174 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('s', 'i') with frequency 5\n",
      "  Step 2.6: Storing merge rule (Priority: 20)...\n",
      "  Stored: ('s', 'i') -> Priority 20\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'si'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'si' to vocabulary. Current size: 52\n",
      "\n",
      "Iteration 22/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 176 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('h', 'er</w>') with frequency 5\n",
      "  Step 2.6: Storing merge rule (Priority: 21)...\n",
      "  Stored: ('h', 'er</w>') -> Priority 21\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'her</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'her</w>' to vocabulary. Current size: 53\n",
      "\n",
      "Iteration 23/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 175 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('h', 'a') with frequency 5\n",
      "  Step 2.6: Storing merge rule (Priority: 22)...\n",
      "  Stored: ('h', 'a') -> Priority 22\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'ha'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'ha' to vocabulary. Current size: 54\n",
      "\n",
      "Iteration 24/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 175 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('o', 'r') with frequency 5\n",
      "  Step 2.6: Storing merge rule (Priority: 23)...\n",
      "  Stored: ('o', 'r') -> Priority 23\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'or'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'or' to vocabulary. Current size: 55\n",
      "\n",
      "Iteration 25/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 175 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('o', 'u') with frequency 5\n",
      "  Step 2.6: Storing merge rule (Priority: 24)...\n",
      "  Stored: ('o', 'u') -> Priority 24\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'ou'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'ou' to vocabulary. Current size: 56\n",
      "\n",
      "Iteration 26/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 175 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('v', 'er') with frequency 4\n",
      "  Step 2.6: Storing merge rule (Priority: 25)...\n",
      "  Stored: ('v', 'er') -> Priority 25\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'ver'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'ver' to vocabulary. Current size: 57\n",
      "\n",
      "Iteration 27/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 174 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('r', 'e') with frequency 4\n",
      "  Step 2.6: Storing merge rule (Priority: 26)...\n",
      "  Stored: ('r', 'e') -> Priority 26\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 're'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 're' to vocabulary. Current size: 58\n",
      "\n",
      "Iteration 28/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 177 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('k', '</w>') with frequency 4\n",
      "  Step 2.6: Storing merge rule (Priority: 27)...\n",
      "  Stored: ('k', '</w>') -> Priority 27\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'k</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'k</w>' to vocabulary. Current size: 59\n",
      "\n",
      "Iteration 29/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 176 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('an', 'd</w>') with frequency 4\n",
      "  Step 2.6: Storing merge rule (Priority: 28)...\n",
      "  Stored: ('an', 'd</w>') -> Priority 28\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'and</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'and</w>' to vocabulary. Current size: 60\n",
      "\n",
      "Iteration 30/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 175 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('or', '</w>') with frequency 4\n",
      "  Step 2.6: Storing merge rule (Priority: 29)...\n",
      "  Stored: ('or', '</w>') -> Priority 29\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'or</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'or</w>' to vocabulary. Current size: 61\n",
      "\n",
      "Iteration 31/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 174 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: (\"'\", '</w>') with frequency 4\n",
      "  Step 2.6: Storing merge rule (Priority: 30)...\n",
      "  Stored: (\"'\", '</w>') -> Priority 30\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: ''</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added ''</w>' to vocabulary. Current size: 62\n",
      "\n",
      "Iteration 32/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 173 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('ic', 'e</w>') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 31)...\n",
      "  Stored: ('ic', 'e</w>') -> Priority 31\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'ice</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'ice</w>' to vocabulary. Current size: 63\n",
      "\n",
      "Iteration 33/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 172 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('w', 'as</w>') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 32)...\n",
      "  Stored: ('w', 'as</w>') -> Priority 32\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'was</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'was</w>' to vocabulary. Current size: 64\n",
      "\n",
      "Iteration 34/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 171 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('t', 'o</w>') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 33)...\n",
      "  Stored: ('t', 'o</w>') -> Priority 33\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'to</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'to</w>' to vocabulary. Current size: 65\n",
      "\n",
      "Iteration 35/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 170 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('t', 'i') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 34)...\n",
      "  Stored: ('t', 'i') -> Priority 34\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'ti'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'ti' to vocabulary. Current size: 66\n",
      "\n",
      "Iteration 36/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 169 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('s', 't') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 35)...\n",
      "  Stored: ('s', 't') -> Priority 35\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'st'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'st' to vocabulary. Current size: 67\n",
      "\n",
      "Iteration 37/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 169 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('s', 'h') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 36)...\n",
      "  Stored: ('s', 'h') -> Priority 36\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'sh'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'sh' to vocabulary. Current size: 68\n",
      "\n",
      "Iteration 38/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 168 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('sh', 'e</w>') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 37)...\n",
      "  Stored: ('sh', 'e</w>') -> Priority 37\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'she</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'she</w>' to vocabulary. Current size: 69\n",
      "\n",
      "Iteration 39/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 167 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('e', 'e') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 38)...\n",
      "  Stored: ('e', 'e') -> Priority 38\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'ee'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'ee' to vocabulary. Current size: 70\n",
      "\n",
      "Iteration 40/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 169 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('i', 't</w>') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 39)...\n",
      "  Stored: ('i', 't</w>') -> Priority 39\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'it</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'it</w>' to vocabulary. Current size: 71\n",
      "\n",
      "Iteration 41/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 168 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('p', 'ic') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 40)...\n",
      "  Stored: ('p', 'ic') -> Priority 40\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'pic'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'pic' to vocabulary. Current size: 72\n",
      "\n",
      "Iteration 42/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 167 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('c', 'on') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 41)...\n",
      "  Stored: ('c', 'on') -> Priority 41\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'con'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'con' to vocabulary. Current size: 73\n",
      "\n",
      "Iteration 43/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 166 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('in', '</w>') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 42)...\n",
      "  Stored: ('in', '</w>') -> Priority 42\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'in</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'in</w>' to vocabulary. Current size: 74\n",
      "\n",
      "Iteration 44/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 165 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('a', '</w>') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 43)...\n",
      "  Stored: ('a', '</w>') -> Priority 43\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'a</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'a</w>' to vocabulary. Current size: 75\n",
      "\n",
      "Iteration 45/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 164 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('d', 'a') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 44)...\n",
      "  Stored: ('d', 'a') -> Priority 44\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'da'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'da' to vocabulary. Current size: 76\n",
      "\n",
      "Iteration 46/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 163 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('w', 'h') with frequency 3\n",
      "  Step 2.6: Storing merge rule (Priority: 45)...\n",
      "  Stored: ('w', 'h') -> Priority 45\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'wh'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'wh' to vocabulary. Current size: 77\n",
      "\n",
      "Iteration 47/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 162 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('a', 'l') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 46)...\n",
      "  Stored: ('a', 'l') -> Priority 46\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'al'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'al' to vocabulary. Current size: 78\n",
      "\n",
      "Iteration 48/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 161 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('al', 'ice</w>') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 47)...\n",
      "  Stored: ('al', 'ice</w>') -> Priority 47\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'alice</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'alice</w>' to vocabulary. Current size: 79\n",
      "\n",
      "Iteration 49/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 160 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('g', 'e') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 48)...\n",
      "  Stored: ('g', 'e') -> Priority 48\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'ge'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'ge' to vocabulary. Current size: 80\n",
      "\n",
      "Iteration 50/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 159 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('ver', 'y</w>') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 49)...\n",
      "  Stored: ('ver', 'y</w>') -> Priority 49\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'very</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'very</w>' to vocabulary. Current size: 81\n",
      "\n",
      "Iteration 51/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 158 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('t', 't') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 50)...\n",
      "  Stored: ('t', 't') -> Priority 50\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'tt'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'tt' to vocabulary. Current size: 82\n",
      "\n",
      "Iteration 52/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 157 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('tt', 'ing</w>') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 51)...\n",
      "  Stored: ('tt', 'ing</w>') -> Priority 51\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'tting</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'tting</w>' to vocabulary. Current size: 83\n",
      "\n",
      "Iteration 53/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 156 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('b', 'y</w>') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 52)...\n",
      "  Stored: ('b', 'y</w>') -> Priority 52\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'by</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'by</w>' to vocabulary. Current size: 84\n",
      "\n",
      "Iteration 54/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 155 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('si', 'st') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 53)...\n",
      "  Stored: ('si', 'st') -> Priority 53\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'sist'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'sist' to vocabulary. Current size: 85\n",
      "\n",
      "Iteration 55/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 154 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('sist', 'er</w>') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 54)...\n",
      "  Stored: ('sist', 'er</w>') -> Priority 54\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'sister</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'sister</w>' to vocabulary. Current size: 86\n",
      "\n",
      "Iteration 56/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 153 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('on', '</w>') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 55)...\n",
      "  Stored: ('on', '</w>') -> Priority 55\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'on</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'on</w>' to vocabulary. Current size: 87\n",
      "\n",
      "Iteration 57/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 153 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('ha', 'd</w>') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 56)...\n",
      "  Stored: ('ha', 'd</w>') -> Priority 56\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'had</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'had</w>' to vocabulary. Current size: 88\n",
      "\n",
      "Iteration 58/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 152 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('ee', 'p') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 57)...\n",
      "  Stored: ('ee', 'p') -> Priority 57\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'eep'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'eep' to vocabulary. Current size: 89\n",
      "\n",
      "Iteration 59/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 151 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('b', 'o') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 58)...\n",
      "  Stored: ('b', 'o') -> Priority 58\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'bo'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'bo' to vocabulary. Current size: 90\n",
      "\n",
      "Iteration 60/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 150 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('bo', 'o') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 59)...\n",
      "  Stored: ('bo', 'o') -> Priority 59\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'boo'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'boo' to vocabulary. Current size: 91\n",
      "\n",
      "Iteration 61/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 149 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('boo', 'k</w>') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 60)...\n",
      "  Stored: ('boo', 'k</w>') -> Priority 60\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'book</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'book</w>' to vocabulary. Current size: 92\n",
      "\n",
      "Iteration 62/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 148 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('a', 'd') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 61)...\n",
      "  Stored: ('a', 'd') -> Priority 61\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'ad'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'ad' to vocabulary. Current size: 93\n",
      "\n",
      "Iteration 63/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 148 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('pic', 't') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 62)...\n",
      "  Stored: ('pic', 't') -> Priority 62\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'pict'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'pict' to vocabulary. Current size: 94\n",
      "\n",
      "Iteration 64/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 147 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('pict', 'u') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 63)...\n",
      "  Stored: ('pict', 'u') -> Priority 63\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'pictu'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'pictu' to vocabulary. Current size: 95\n",
      "\n",
      "Iteration 65/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 146 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('pictu', 're') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 64)...\n",
      "  Stored: ('pictu', 're') -> Priority 64\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'picture'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'picture' to vocabulary. Current size: 96\n",
      "\n",
      "Iteration 66/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 145 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('picture', 's</w>') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 65)...\n",
      "  Stored: ('picture', 's</w>') -> Priority 65\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'pictures</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'pictures</w>' to vocabulary. Current size: 97\n",
      "\n",
      "Iteration 67/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 144 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('con', 'ver') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 66)...\n",
      "  Stored: ('con', 'ver') -> Priority 66\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'conver'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'conver' to vocabulary. Current size: 98\n",
      "\n",
      "Iteration 68/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 143 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('conver', 's') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 67)...\n",
      "  Stored: ('conver', 's') -> Priority 67\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'convers'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'convers' to vocabulary. Current size: 99\n",
      "\n",
      "Iteration 69/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 142 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('convers', 'a') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 68)...\n",
      "  Stored: ('convers', 'a') -> Priority 68\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'conversa'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'conversa' to vocabulary. Current size: 100\n",
      "\n",
      "Iteration 70/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 141 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('conversa', 'ti') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 69)...\n",
      "  Stored: ('conversa', 'ti') -> Priority 69\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'conversati'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'conversati' to vocabulary. Current size: 101\n",
      "\n",
      "Iteration 71/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 140 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('s', 'e</w>') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 70)...\n",
      "  Stored: ('s', 'e</w>') -> Priority 70\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'se</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'se</w>' to vocabulary. Current size: 102\n",
      "\n",
      "Iteration 72/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 139 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('th', 'ou') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 71)...\n",
      "  Stored: ('th', 'ou') -> Priority 71\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'thou'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'thou' to vocabulary. Current size: 103\n",
      "\n",
      "Iteration 73/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 139 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('w', 'i') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 72)...\n",
      "  Stored: ('w', 'i') -> Priority 72\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'wi'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'wi' to vocabulary. Current size: 104\n",
      "\n",
      "Iteration 74/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 138 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('n', '</w>') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 73)...\n",
      "  Stored: ('n', '</w>') -> Priority 73\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'n</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'n</w>' to vocabulary. Current size: 105\n",
      "\n",
      "Iteration 75/75\n",
      "  Step 2.3: Calculating pair statistics...\n",
      "  Calculated frequencies for 138 unique pairs.\n",
      "  Step 2.4: Checking if pairs exist...\n",
      "  Pairs found, continuing training.\n",
      "  Step 2.5: Finding the most frequent pair...\n",
      "  Found best pair: ('l', '</w>') with frequency 2\n",
      "  Step 2.6: Storing merge rule (Priority: 74)...\n",
      "  Stored: ('l', '</w>') -> Priority 74\n",
      "  Step 2.7: Creating new symbol from best pair...\n",
      "  New symbol created: 'l</w>'\n",
      "  Step 2.8: Updating corpus representation...\n",
      "  Corpus representation updated for all words.\n",
      "  Step 2.9: Updating vocabulary...\n",
      "  Added 'l</w>' to vocabulary. Current size: 106\n",
      "\n",
      "--- BPE Training Loop Finished after 75 iterations (or target reached) ---\n",
      "Final vocabulary, merge rules, and corpus representation are ready.\n"
     ]
    }
   ],
   "source": [
    "# Start the main loop that iterates for the specified number of merges\n",
    "print(f\"\\n--- Starting BPE Training Loop ({num_merges} iterations) ---\")\n",
    "for i in range(num_merges):\n",
    "    # --- Code for steps 2.3 to 2.9 will go inside this loop --- \n",
    "    # Print the current iteration number (starting from 1)\n",
    "    print(f\"\\nIteration {i + 1}/{num_merges}\")\n",
    "\n",
    "    # --- Step 2.3 (Inside Loop): Calculate Pair Statistics --- \n",
    "    # Theory: We must recalculate pair frequencies in *every* iteration because the \n",
    "    # corpus representation changes after each merge. A pair that was frequent might \n",
    "    # become less frequent after its components are merged into a new symbol.\n",
    "    # We iterate through each unique *original* word and its frequency. For each word,\n",
    "    # we get its *current* symbol representation from `current_corpus_split`. We then\n",
    "    # iterate through adjacent symbols in that representation and increment the count\n",
    "    # for that pair in `pair_counts`, weighted by the original word's frequency.\n",
    "    print(\"  Step 2.3: Calculating pair statistics...\")\n",
    "    pair_counts = collections.Counter()\n",
    "    # Iterate through the original words and their frequencies\n",
    "    for word, freq in word_frequencies.items():\n",
    "        # Get the *current* list of symbols for this word from the evolving representation\n",
    "        symbols = current_corpus_split[word]\n",
    "        # Iterate through the adjacent pairs in the current symbol list\n",
    "        # Loop from the first symbol up to the second-to-last symbol\n",
    "        for j in range(len(symbols) - 1):\n",
    "            # Form the pair (tuple of two adjacent symbols)\n",
    "            pair = (symbols[j], symbols[j+1])\n",
    "            # Increment the count for this pair by the frequency of the original word\n",
    "            pair_counts[pair] += freq \n",
    "    print(f\"  Calculated frequencies for {len(pair_counts)} unique pairs.\")\n",
    "    # Optional: print top 5 pairs found in this iteration\n",
    "    # if pair_counts:\n",
    "    #     print(f\"    Top 5 pairs this iteration: {pair_counts.most_common(5)}\")\n",
    "\n",
    "    # --- Step 2.4 (Inside Loop): Check for Termination Condition ---\n",
    "    # Theory: If, after counting, `pair_counts` is empty, it means there are no \n",
    "    # adjacent pairs left in any word representation in the corpus. This can happen \n",
    "    # if the corpus is very small or if all possible merges have effectively been done.\n",
    "    # In this case, we can't proceed, so we break out of the training loop early.\n",
    "    print(\"  Step 2.4: Checking if pairs exist...\")\n",
    "    if not pair_counts:\n",
    "        print(\"  No more pairs found to merge. Stopping training loop early.\")\n",
    "        break # Exit the 'for i in range(num_merges)' loop\n",
    "    print(\"  Pairs found, continuing training.\")\n",
    "\n",
    "    # --- Step 2.5 (Inside Loop): Find the Best Pair --- \n",
    "    # Theory: We need to find the key (the pair tuple) in the `pair_counts` counter \n",
    "    # that has the maximum associated value (the frequency count). The `max()` function \n",
    "    # can take a `key` argument, which specifies a function to determine the sorting/comparison value.\n",
    "    # `pair_counts.get` is a function that returns the value for a given key in the counter.\n",
    "    # So, `max(pair_counts, key=pair_counts.get)` finds the pair with the highest count.\n",
    "    print(\"  Step 2.5: Finding the most frequent pair...\")\n",
    "    try:\n",
    "        best_pair = max(pair_counts, key=pair_counts.get)\n",
    "        best_pair_frequency = pair_counts[best_pair]\n",
    "        print(f\"  Found best pair: {best_pair} with frequency {best_pair_frequency}\")\n",
    "    except ValueError:\n",
    "        # This should theoretically be caught by the 'if not pair_counts' check above,\n",
    "        # but adding robust error handling is good practice.\n",
    "        print(\"  Error: Could not find maximum in empty pair_counts. Stopping.\")\n",
    "        break\n",
    "\n",
    "    # --- Step 2.6 (Inside Loop): Store Merge Rule ---\n",
    "    # Theory: We record which pair was chosen to be merged in this iteration. The priority\n",
    "    # is simply the iteration index `i`. This ordered list of merges is crucial for \n",
    "    # correctly tokenizing new text later.\n",
    "    print(f\"  Step 2.6: Storing merge rule (Priority: {i})...\")\n",
    "    learned_merges[best_pair] = i\n",
    "    print(f\"  Stored: {best_pair} -> Priority {i}\")\n",
    "\n",
    "    # --- Step 2.7 (Inside Loop): Create New Symbol ---\n",
    "    # Theory: The new symbol representing the merged pair is created by simply \n",
    "    # concatenating the string representations of the two symbols in the pair.\n",
    "    print(\"  Step 2.7: Creating new symbol from best pair...\")\n",
    "    new_symbol = \"\".join(best_pair)\n",
    "    print(f\"  New symbol created: '{new_symbol}'\")\n",
    "\n",
    "    # --- Step 2.8 (Inside Loop): Update Corpus Representation ---\n",
    "    # Theory: This is the most complex part of the loop. We must go through every\n",
    "    # word representation in `current_corpus_split` and replace *all* occurrences\n",
    "    # of the `best_pair` sequence with the `new_symbol`. It's essential to create\n",
    "    # a *new* dictionary (`next_corpus_split`) to store these results, rather than\n",
    "    # modifying `current_corpus_split` in place while iterating over it. \n",
    "    # For each word, we scan its current symbol list (`old_symbols`). We build a \n",
    "    # `new_symbols` list. If we encounter the `best_pair` starting at index `k`, \n",
    "    # we append `new_symbol` to `new_symbols` and advance our scan position `k` by 2.\n",
    "    # Otherwise, we just append the symbol `old_symbols[k]` and advance `k` by 1.\n",
    "    # After processing all words, `current_corpus_split` is updated to become \n",
    "    # `next_corpus_split` for the *next* training iteration.\n",
    "    print(\"  Step 2.8: Updating corpus representation...\")\n",
    "    next_corpus_split = {}\n",
    "    # Iterate through all original words (keys in the current split dictionary)\n",
    "    for word in current_corpus_split:\n",
    "        # Get the list of symbols for this word *before* applying the current merge\n",
    "        old_symbols = current_corpus_split[word]\n",
    "        # Initialize an empty list to build the new sequence of symbols for this word\n",
    "        new_symbols = []\n",
    "        # Initialize scan index for the old_symbols list\n",
    "        k = 0\n",
    "        # Scan through the old symbols list\n",
    "        while k < len(old_symbols):\n",
    "            # Check if we are not at the very last symbol (to allow pair formation)\n",
    "            # and if the pair starting at index k matches the best_pair to be merged\n",
    "            if k < len(old_symbols) - 1 and (old_symbols[k], old_symbols[k+1]) == best_pair:\n",
    "                # If match found, append the new merged symbol to our new list\n",
    "                new_symbols.append(new_symbol)\n",
    "                # Advance the scan index by 2 (skipping both parts of the merged pair)\n",
    "                k += 2\n",
    "            else:\n",
    "                # If no match, just append the current symbol from the old list\n",
    "                new_symbols.append(old_symbols[k])\n",
    "                # Advance the scan index by 1\n",
    "                k += 1\n",
    "        # Store the newly constructed symbol list for this word in the temporary dictionary\n",
    "        next_corpus_split[word] = new_symbols\n",
    "        \n",
    "    # After processing all words, update the main corpus split to reflect the merge\n",
    "    current_corpus_split = next_corpus_split\n",
    "    print(\"  Corpus representation updated for all words.\")\n",
    "    # Optional: Show how a specific word changed after this merge\n",
    "    # if 'beginning' in current_corpus_split:\n",
    "    #     print(f\"    Example 'beginning' now: {current_corpus_split['beginning']}\")\n",
    "\n",
    "    # --- Step 2.9 (Inside Loop): Update Vocabulary ---\n",
    "    # Theory: Add the newly created `new_symbol` (the result of the merge)\n",
    "    # to the `current_vocab` set. Sets automatically handle duplicates.\n",
    "    print(\"  Step 2.9: Updating vocabulary...\")\n",
    "    current_vocab.add(new_symbol)\n",
    "    print(f\"  Added '{new_symbol}' to vocabulary. Current size: {len(current_vocab)}\")\n",
    "\n",
    "# --- Step 2.10: Training Loop Finished ---\n",
    "# Theory: Once the loop completes (either reaches `num_merges` or breaks early),\n",
    "# the training process is finished. The final learned state is contained in \n",
    "# `learned_merges`, `current_vocab`, and `current_corpus_split`.\n",
    "print(f\"\\n--- BPE Training Loop Finished after {i + 1} iterations (or target reached) ---\")\n",
    "\n",
    "# Assign final state variables for clarity (optional, could just use current_*)\n",
    "final_vocabulary = current_vocab\n",
    "final_learned_merges = learned_merges\n",
    "final_corpus_representation = current_corpus_split\n",
    "\n",
    "print(\"Final vocabulary, merge rules, and corpus representation are ready.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Inspect Training Results\n",
    "\n",
    "**Goal:** Examine the artifacts produced by the BPE training process to understand what was learned."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Final Vocabulary Size\n",
    "\n",
    "**Theory:** The final vocabulary contains all initial characters plus all the new subword symbols created during the merge process. Its size is a key outcome of the training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Inspecting Training Results ---\n",
      "Final Vocabulary Size: 106 tokens\n"
     ]
    }
   ],
   "source": [
    "print(\"--- Inspecting Training Results ---\")\n",
    "print(f\"Final Vocabulary Size: {len(final_vocabulary)} tokens\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: Learned Merge Rules (Sorted by Priority)\n",
    "\n",
    "**Theory:** The `final_learned_merges` dictionary contains the crucial information about *which* pairs were merged and *in what order* (priority). To understand the process and apply it correctly during tokenization, it's helpful to view these merges sorted by their priority value (the iteration index `i`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Learned Merge Rules (Sorted by Priority):\n",
      "Total merges learned: 75\n",
      "  (Showing first 10 and last 10 merges)\n",
      "  Priority 0: ('e', '</w>') -> 'e</w>'\n",
      "  Priority 1: ('i', 'n') -> 'in'\n",
      "  Priority 2: ('e', 'r') -> 'er'\n",
      "  Priority 3: ('t', 'h') -> 'th'\n",
      "  Priority 4: ('d', '</w>') -> 'd</w>'\n",
      "  Priority 5: ('s', '</w>') -> 's</w>'\n",
      "  Priority 6: ('in', 'g') -> 'ing'\n",
      "  Priority 7: ('ing', '</w>') -> 'ing</w>'\n",
      "  Priority 8: ('t', '</w>') -> 't</w>'\n",
      "  Priority 9: ('y', '</w>') -> 'y</w>'\n",
      "  ...\n",
      "  Priority 65: ('picture', 's</w>') -> 'pictures</w>'\n",
      "  Priority 66: ('con', 'ver') -> 'conver'\n",
      "  Priority 67: ('conver', 's') -> 'convers'\n",
      "  Priority 68: ('convers', 'a') -> 'conversa'\n",
      "  Priority 69: ('conversa', 'ti') -> 'conversati'\n",
      "  Priority 70: ('s', 'e</w>') -> 'se</w>'\n",
      "  Priority 71: ('th', 'ou') -> 'thou'\n",
      "  Priority 72: ('w', 'i') -> 'wi'\n",
      "  Priority 73: ('n', '</w>') -> 'n</w>'\n",
      "  Priority 74: ('l', '</w>') -> 'l</w>'\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nLearned Merge Rules (Sorted by Priority):\")\n",
    "\n",
    "# Convert the dictionary items to a list of (pair, priority) tuples\n",
    "merges_list = list(final_learned_merges.items())\n",
    "\n",
    "# Sort the list based on the priority (the second element of the tuple, index 1)\n",
    "# `lambda item: item[1]` tells sort to use the priority value for sorting\n",
    "sorted_merges_list = sorted(merges_list, key=lambda item: item[1])\n",
    "\n",
    "# Display the sorted merges\n",
    "print(f\"Total merges learned: {len(sorted_merges_list)}\")\n",
    "# Print a sample if the list is long, otherwise print all\n",
    "display_limit = 20 \n",
    "if len(sorted_merges_list) <= display_limit * 2:\n",
    "    for pair, priority in sorted_merges_list:\n",
    "        print(f\"  Priority {priority}: {pair} -> '{''.join(pair)}'\")\n",
    "else:\n",
    "    print(\"  (Showing first 10 and last 10 merges)\")\n",
    "    # Print first N\n",
    "    for pair, priority in sorted_merges_list[:display_limit // 2]:\n",
    "        print(f\"  Priority {priority}: {pair} -> '{''.join(pair)}'\")\n",
    "    print(\"  ...\")\n",
    "    # Print last N\n",
    "    for pair, priority in sorted_merges_list[-display_limit // 2:]:\n",
    "        print(f\"  Priority {priority}: {pair} -> '{''.join(pair)}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.3: Final Representation of Example Words\n",
    "\n",
    "**Theory:** It's useful to see how specific words from the training corpus are represented *after* all the merge operations have been applied. This shows the final token sequence for known words according to the learned BPE rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final Representation of Example Words from Training Corpus:\n",
      "  'beginning': ['b', 'e', 'g', 'in', 'n', 'ing</w>']\n",
      "  'conversations': ['conversati', 'on', 's</w>']\n",
      "  'sister': ['sister</w>']\n",
      "  'pictures': ['pictures</w>']\n",
      "  'reading': ['re', 'ad', 'ing</w>']\n",
      "  'alice': ['alice</w>']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nFinal Representation of Example Words from Training Corpus:\")\n",
    "\n",
    "# List some words we expect to see interesting tokenization for\n",
    "example_words_to_inspect = ['beginning', 'conversations', 'sister', 'pictures', 'reading', 'alice']\n",
    "\n",
    "for word in example_words_to_inspect:\n",
    "    if word in final_corpus_representation:\n",
    "        print(f\"  '{word}': {final_corpus_representation[word]}\")\n",
    "    else:\n",
    "        print(f\"  '{word}': Not found in original corpus (should not happen if chosen from corpus).\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Tokenization of New Text using Learned Rules\n",
    "\n",
    "**Goal:** Apply the BPE rules learned during training (`final_learned_merges`) to segment a new, unseen piece of text into a sequence of tokens (characters and subwords from `final_vocabulary`)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Define New Text Input\n",
    "\n",
    "**Theory:** We need a sample sentence or text that the BPE model hasn't necessarily seen during training to demonstrate its ability to tokenize new input, potentially including words not in the original corpus."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--- Tokenizing New Text ---\n",
      "Input Text: 'Alice thought reading was tiresome without pictures.'\n"
     ]
    }
   ],
   "source": [
    "# Define a new text string to tokenize using the learned BPE rules\n",
    "# This text contains words seen in training ('alice', 'pictures') \n",
    "# and potentially unseen words or variations ('tiresome', 'thought')\n",
    "new_text_to_tokenize = \"Alice thought reading was tiresome without pictures.\"\n",
    "\n",
    "print(f\"--- Tokenizing New Text ---\")\n",
    "print(f\"Input Text: '{new_text_to_tokenize}'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Preprocess New Text\n",
    "\n",
    "**Theory:** It is absolutely critical that the new text undergoes the *exact same* initial preprocessing steps as the training data. This includes lowercasing and splitting using the same method (here, the same regular expression)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4.2: Preprocessing the new text...\n",
      "  Lowercased: 'alice thought reading was tiresome without pictures.'\n",
      "  Split into words/tokens: ['alice', 'thought', 'reading', 'was', 'tiresome', 'without', 'pictures', '.']\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4.2: Preprocessing the new text...\")\n",
    "# 1. Lowercase the new text\n",
    "new_text_lower = new_text_to_tokenize.lower()\n",
    "print(f\"  Lowercased: '{new_text_lower}'\")\n",
    "\n",
    "# 2. Split into words/tokens using the same regex as in training\n",
    "# Recall: split_pattern = r'\\w+|[^\\s\\w]+'\n",
    "new_words_list = re.findall(split_pattern, new_text_lower)\n",
    "print(f\"  Split into words/tokens: {new_words_list}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3: Prepare for Tokenization Output\n",
    "\n",
    "**Theory:** We initialize an empty list that will accumulate the final sequence of tokens for the entire input text as we process each word individually."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4.3: Initialized empty list for tokenized output.\n"
     ]
    }
   ],
   "source": [
    "# Initialize an empty list to store the final sequence of tokens for the whole text\n",
    "tokenized_output = []\n",
    "print(\"Step 4.3: Initialized empty list for tokenized output.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.4: Iterate Through Words in New Text\n",
    "\n",
    "**Theory:** We now process each word/token obtained from the preprocessing step (`new_words_list`) one by one. For each word, we will apply the learned BPE merge rules."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step 4.4: Starting iteration through words of the new text...\n",
      "\n",
      "  Processing Word: 'alice'\n",
      "    Step 4.5: Initializing symbols for this word...\n",
      "      Initial symbols: ['a', 'l', 'i', 'c', 'e', '</w>']\n",
      "    Step 4.6: Applying learned merges iteratively...\n",
      "      Applying highest priority merge: ('e', '</w>') (Priority 0) at index 4 -> 'e</w>'\n",
      "      Updated symbols: ['a', 'l', 'i', 'c', 'e</w>']\n",
      "      Applying highest priority merge: ('i', 'c') (Priority 13) at index 2 -> 'ic'\n",
      "      Updated symbols: ['a', 'l', 'ic', 'e</w>']\n",
      "      Applying highest priority merge: ('ic', 'e</w>') (Priority 31) at index 2 -> 'ice</w>'\n",
      "      Updated symbols: ['a', 'l', 'ice</w>']\n",
      "      Applying highest priority merge: ('a', 'l') (Priority 46) at index 0 -> 'al'\n",
      "      Updated symbols: ['al', 'ice</w>']\n",
      "      Applying highest priority merge: ('al', 'ice</w>') (Priority 47) at index 0 -> 'alice</w>'\n",
      "      Updated symbols: ['alice</w>']\n",
      "      No more applicable learned merges found for this word.\n",
      "    Step 4.7: Appending final tokens for this word to overall output...\n",
      "      Appended: ['alice</w>']\n",
      "\n",
      "  Processing Word: 'thought'\n",
      "    Step 4.5: Initializing symbols for this word...\n",
      "      Initial symbols: ['t', 'h', 'o', 'u', 'g', 'h', 't', '</w>']\n",
      "    Step 4.6: Applying learned merges iteratively...\n",
      "      Applying highest priority merge: ('t', 'h') (Priority 3) at index 0 -> 'th'\n",
      "      Updated symbols: ['th', 'o', 'u', 'g', 'h', 't', '</w>']\n",
      "      Applying highest priority merge: ('t', '</w>') (Priority 8) at index 5 -> 't</w>'\n",
      "      Updated symbols: ['th', 'o', 'u', 'g', 'h', 't</w>']\n",
      "      Applying highest priority merge: ('o', 'u') (Priority 24) at index 1 -> 'ou'\n",
      "      Updated symbols: ['th', 'ou', 'g', 'h', 't</w>']\n",
      "      Applying highest priority merge: ('th', 'ou') (Priority 71) at index 0 -> 'thou'\n",
      "      Updated symbols: ['thou', 'g', 'h', 't</w>']\n",
      "      No more applicable learned merges found for this word.\n",
      "    Step 4.7: Appending final tokens for this word to overall output...\n",
      "      Appended: ['thou', 'g', 'h', 't</w>']\n",
      "\n",
      "  Processing Word: 'reading'\n",
      "    Step 4.5: Initializing symbols for this word...\n",
      "      Initial symbols: ['r', 'e', 'a', 'd', 'i', 'n', 'g', '</w>']\n",
      "    Step 4.6: Applying learned merges iteratively...\n",
      "      Applying highest priority merge: ('i', 'n') (Priority 1) at index 4 -> 'in'\n",
      "      Updated symbols: ['r', 'e', 'a', 'd', 'in', 'g', '</w>']\n",
      "      Applying highest priority merge: ('in', 'g') (Priority 6) at index 4 -> 'ing'\n",
      "      Updated symbols: ['r', 'e', 'a', 'd', 'ing', '</w>']\n",
      "      Applying highest priority merge: ('ing', '</w>') (Priority 7) at index 4 -> 'ing</w>'\n",
      "      Updated symbols: ['r', 'e', 'a', 'd', 'ing</w>']\n",
      "      Applying highest priority merge: ('r', 'e') (Priority 26) at index 0 -> 're'\n",
      "      Updated symbols: ['re', 'a', 'd', 'ing</w>']\n",
      "      Applying highest priority merge: ('a', 'd') (Priority 61) at index 1 -> 'ad'\n",
      "      Updated symbols: ['re', 'ad', 'ing</w>']\n",
      "      No more applicable learned merges found for this word.\n",
      "    Step 4.7: Appending final tokens for this word to overall output...\n",
      "      Appended: ['re', 'ad', 'ing</w>']\n",
      "\n",
      "  Processing Word: 'was'\n",
      "    Step 4.5: Initializing symbols for this word...\n",
      "      Initial symbols: ['w', 'a', 's', '</w>']\n",
      "    Step 4.6: Applying learned merges iteratively...\n",
      "      Applying highest priority merge: ('s', '</w>') (Priority 5) at index 2 -> 's</w>'\n",
      "      Updated symbols: ['w', 'a', 's</w>']\n",
      "      Applying highest priority merge: ('a', 's</w>') (Priority 17) at index 1 -> 'as</w>'\n",
      "      Updated symbols: ['w', 'as</w>']\n",
      "      Applying highest priority merge: ('w', 'as</w>') (Priority 32) at index 0 -> 'was</w>'\n",
      "      Updated symbols: ['was</w>']\n",
      "      No more applicable learned merges found for this word.\n",
      "    Step 4.7: Appending final tokens for this word to overall output...\n",
      "      Appended: ['was</w>']\n",
      "\n",
      "  Processing Word: 'tiresome'\n",
      "    Step 4.5: Initializing symbols for this word...\n",
      "      Initial symbols: ['t', 'i', 'r', 'e', 's', 'o', 'm', 'e', '</w>']\n",
      "    Step 4.6: Applying learned merges iteratively...\n",
      "      Applying highest priority merge: ('e', '</w>') (Priority 0) at index 7 -> 'e</w>'\n",
      "      Updated symbols: ['t', 'i', 'r', 'e', 's', 'o', 'm', 'e</w>']\n",
      "      Applying highest priority merge: ('r', 'e') (Priority 26) at index 2 -> 're'\n",
      "      Updated symbols: ['t', 'i', 're', 's', 'o', 'm', 'e</w>']\n",
      "      Applying highest priority merge: ('t', 'i') (Priority 34) at index 0 -> 'ti'\n",
      "      Updated symbols: ['ti', 're', 's', 'o', 'm', 'e</w>']\n",
      "      No more applicable learned merges found for this word.\n",
      "    Step 4.7: Appending final tokens for this word to overall output...\n",
      "      Appended: ['ti', 're', 's', 'o', 'm', 'e</w>']\n",
      "\n",
      "  Processing Word: 'without'\n",
      "    Step 4.5: Initializing symbols for this word...\n",
      "      Initial symbols: ['w', 'i', 't', 'h', 'o', 'u', 't', '</w>']\n",
      "    Step 4.6: Applying learned merges iteratively...\n",
      "      Applying highest priority merge: ('t', 'h') (Priority 3) at index 2 -> 'th'\n",
      "      Updated symbols: ['w', 'i', 'th', 'o', 'u', 't', '</w>']\n",
      "      Applying highest priority merge: ('t', '</w>') (Priority 8) at index 5 -> 't</w>'\n",
      "      Updated symbols: ['w', 'i', 'th', 'o', 'u', 't</w>']\n",
      "      Applying highest priority merge: ('o', 'u') (Priority 24) at index 3 -> 'ou'\n",
      "      Updated symbols: ['w', 'i', 'th', 'ou', 't</w>']\n",
      "      Applying highest priority merge: ('th', 'ou') (Priority 71) at index 2 -> 'thou'\n",
      "      Updated symbols: ['w', 'i', 'thou', 't</w>']\n",
      "      Applying highest priority merge: ('w', 'i') (Priority 72) at index 0 -> 'wi'\n",
      "      Updated symbols: ['wi', 'thou', 't</w>']\n",
      "      No more applicable learned merges found for this word.\n",
      "    Step 4.7: Appending final tokens for this word to overall output...\n",
      "      Appended: ['wi', 'thou', 't</w>']\n",
      "\n",
      "  Processing Word: 'pictures'\n",
      "    Step 4.5: Initializing symbols for this word...\n",
      "      Initial symbols: ['p', 'i', 'c', 't', 'u', 'r', 'e', 's', '</w>']\n",
      "    Step 4.6: Applying learned merges iteratively...\n",
      "      Applying highest priority merge: ('s', '</w>') (Priority 5) at index 7 -> 's</w>'\n",
      "      Updated symbols: ['p', 'i', 'c', 't', 'u', 'r', 'e', 's</w>']\n",
      "      Applying highest priority merge: ('i', 'c') (Priority 13) at index 1 -> 'ic'\n",
      "      Updated symbols: ['p', 'ic', 't', 'u', 'r', 'e', 's</w>']\n",
      "      Applying highest priority merge: ('r', 'e') (Priority 26) at index 4 -> 're'\n",
      "      Updated symbols: ['p', 'ic', 't', 'u', 're', 's</w>']\n",
      "      Applying highest priority merge: ('p', 'ic') (Priority 40) at index 0 -> 'pic'\n",
      "      Updated symbols: ['pic', 't', 'u', 're', 's</w>']\n",
      "      Applying highest priority merge: ('pic', 't') (Priority 62) at index 0 -> 'pict'\n",
      "      Updated symbols: ['pict', 'u', 're', 's</w>']\n",
      "      Applying highest priority merge: ('pict', 'u') (Priority 63) at index 0 -> 'pictu'\n",
      "      Updated symbols: ['pictu', 're', 's</w>']\n",
      "      Applying highest priority merge: ('pictu', 're') (Priority 64) at index 0 -> 'picture'\n",
      "      Updated symbols: ['picture', 's</w>']\n",
      "      Applying highest priority merge: ('picture', 's</w>') (Priority 65) at index 0 -> 'pictures</w>'\n",
      "      Updated symbols: ['pictures</w>']\n",
      "      No more applicable learned merges found for this word.\n",
      "    Step 4.7: Appending final tokens for this word to overall output...\n",
      "      Appended: ['pictures</w>']\n",
      "\n",
      "  Processing Word: '.'\n",
      "    Step 4.5: Initializing symbols for this word...\n",
      "      Initial symbols: ['.', '</w>']\n",
      "    Step 4.6: Applying learned merges iteratively...\n",
      "      No more applicable learned merges found for this word.\n",
      "    Step 4.7: Appending final tokens for this word to overall output...\n",
      "      Appended: ['.', '</w>']\n",
      "\n",
      "Finished processing all words in the new text.\n"
     ]
    }
   ],
   "source": [
    "print(\"Step 4.4: Starting iteration through words of the new text...\")\n",
    "# Loop through each preprocessed word/token from the new text\n",
    "for word in new_words_list:\n",
    "    print(f\"\\n  Processing Word: '{word}'\")\n",
    "    \n",
    "    # --- Steps 4.5 to 4.7 will go inside this loop --- \n",
    "    \n",
    "    # --- Step 4.5 (Inside Word Loop): Initialize Word Symbols ---\n",
    "    # Theory: Just like in training initialization, represent the current word as \n",
    "    # a list of its characters plus the end-of-word symbol. This is the starting \n",
    "    # point for applying merges to this specific word.\n",
    "    print(\"    Step 4.5: Initializing symbols for this word...\")\n",
    "    word_symbols = list(word) + [end_of_word_symbol] # Recall: end_of_word_symbol = '</w>'\n",
    "    print(f\"      Initial symbols: {word_symbols}\")\n",
    "    \n",
    "    # --- Step 4.6 (Inside Word Loop): Inner Loop for Applying Merges ---\n",
    "    # Theory: This is the core tokenization logic for a *single* word. We need to \n",
    "    # repeatedly apply the learned merge rules from `final_learned_merges` to the \n",
    "    # `word_symbols` list. The crucial rule is: in each pass, find *all* possible \n",
    "    # merges that apply to the current `word_symbols`, but only execute the one \n",
    "    # that has the *highest priority* (i.e., the lowest merge index stored in \n",
    "    # `final_learned_merges`). After applying that single best merge, the `word_symbols` \n",
    "    # list is updated, and we *repeat* the process: scan the *new* list again for \n",
    "    # the highest-priority applicable merge. This continues until a pass completes \n",
    "    # where no learned merges can be applied to the current `word_symbols`.\n",
    "    print(\"    Step 4.6: Applying learned merges iteratively...\")\n",
    "    while True: # Loop until no more merges can be applied to this word\n",
    "        # --- Find the best merge for the CURRENT pass --- \n",
    "        # Initialize variables to track the best merge found *in this pass*\n",
    "        best_priority_found_this_pass = float('inf') # Use infinity to ensure any valid priority is lower\n",
    "        pair_to_merge_this_pass = None\n",
    "        merge_location_this_pass = -1\n",
    "        \n",
    "        # Scan through the current word_symbols list to find applicable merges\n",
    "        # Iterate up to the second-to-last symbol to form pairs\n",
    "        scan_index = 0\n",
    "        while scan_index < len(word_symbols) - 1:\n",
    "            # Form the adjacent pair at the current scan index\n",
    "            current_pair = (word_symbols[scan_index], word_symbols[scan_index + 1])\n",
    "            \n",
    "            # Check if this pair exists in our learned merge rules\n",
    "            if current_pair in final_learned_merges:\n",
    "                # If it exists, get its priority (when it was learned)\n",
    "                current_pair_priority = final_learned_merges[current_pair]\n",
    "                \n",
    "                # Check if this pair's priority is better (lower number) than the best found so far *in this pass*\n",
    "                if current_pair_priority < best_priority_found_this_pass:\n",
    "                    # If yes, update the best merge found for this pass\n",
    "                    best_priority_found_this_pass = current_pair_priority\n",
    "                    pair_to_merge_this_pass = current_pair\n",
    "                    merge_location_this_pass = scan_index # Record where the best merge starts\n",
    "                    \n",
    "            # Move to the next position to check the next pair\n",
    "            scan_index += 1\n",
    "            \n",
    "        # --- Apply the best merge found (if any) or break --- \n",
    "        # After scanning the entire current word_symbols list:\n",
    "        if pair_to_merge_this_pass is not None:\n",
    "            # An applicable merge was found. Apply the one with the highest priority.\n",
    "            merged_symbol = \"\".join(pair_to_merge_this_pass)\n",
    "            print(f\"      Applying highest priority merge: {pair_to_merge_this_pass} (Priority {best_priority_found_this_pass}) at index {merge_location_this_pass} -> '{merged_symbol}'\")\n",
    "            \n",
    "            # Reconstruct the word_symbols list with the merge applied\n",
    "            # Slice before the merge + new symbol + slice after the merge\n",
    "            word_symbols = word_symbols[:merge_location_this_pass] + [merged_symbol] + word_symbols[merge_location_this_pass + 2:]\n",
    "            print(f\"      Updated symbols: {word_symbols}\")\n",
    "            # Continue to the next iteration of the 'while True' loop to scan the *updated* symbols list\n",
    "        else:\n",
    "            # No applicable merges were found in the entire scan of the current symbols list\n",
    "            print(\"      No more applicable learned merges found for this word.\")\n",
    "            break # Exit the 'while True' loop for this word\n",
    "            \n",
    "    # --- Step 4.7 (Inside Word Loop): Append Final Word Tokens ---\n",
    "    # Theory: After the inner 'while True' loop finishes, `word_symbols` contains \n",
    "    # the final sequence of tokens (characters or subwords) for the current word.\n",
    "    # We append these tokens to the main `tokenized_output` list using `extend`.\n",
    "    print(\"    Step 4.7: Appending final tokens for this word to overall output...\")\n",
    "    tokenized_output.extend(word_symbols)\n",
    "    print(f\"      Appended: {word_symbols}\")\n",
    "\n",
    "print(\"\\nFinished processing all words in the new text.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.8: Display Final Tokenization Result\n",
    "\n",
    "**Theory:** Finally, display the original input text alongside the complete list of tokens produced by applying the learned BPE rules. This demonstrates the outcome of the entire tokenization process."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Final Tokenization Result ---\n",
      "Original Input Text: 'Alice thought reading was tiresome without pictures.'\n",
      "Tokenized Output (21 tokens): ['alice</w>', 'thou', 'g', 'h', 't</w>', 're', 'ad', 'ing</w>', 'was</w>', 'ti', 're', 's', 'o', 'm', 'e</w>', 'wi', 'thou', 't</w>', 'pictures</w>', '.', '</w>']\n"
     ]
    }
   ],
   "source": [
    "print(\"\\n--- Final Tokenization Result ---\")\n",
    "print(f\"Original Input Text: '{new_text_to_tokenize}'\")\n",
    "print(f\"Tokenized Output ({len(tokenized_output)} tokens): {tokenized_output}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Conclusion\n",
    "\n",
    "This notebook provided an extremely detailed, step-by-step, inline implementation of the Byte Pair Encoding algorithm for text tokenization, deliberately avoiding the use of functions or classes to expose the fundamental logic at each stage.\n",
    "\n",
    "We meticulously followed the key phases:\n",
    "1.  **Initialization & Preprocessing:** Standardizing the input text (lowercasing), performing initial splitting (words/punctuation), calculating frequencies, representing words as character sequences with crucial `</w>` end-of-word markers, and building the initial character-based vocabulary.\n",
    "2.  **Iterative Training:** Looping for a predefined number of merges (`num_merges`). In each iteration, we precisely counted adjacent symbol pair frequencies across the *current* corpus state (weighted by word frequency), identified the single most frequent pair, recorded this merge operation along with its priority (based on the iteration number), created a new subword symbol, updated the *entire* corpus representation by replacing the merged pair with the new symbol, and added the new symbol to the vocabulary.\n",
    "3.  **Inspection:** We examined the final vocabulary size, the list of learned merge rules sorted by the priority in which they were learned, and the final token representation of sample words from the training corpus.\n",
    "4.  **Tokenization of New Text:** We took an unseen text input, applied the *exact same* preprocessing steps, and then, for each word, iteratively applied the learned merge rules. Crucially, in each tokenization step for a word, we found the merge rule with the *highest priority* (earliest learned) that was applicable to the current sequence of symbols for that word, applied only that merge, and repeated the process until no more learned merges could be applied.\n",
    "\n",
    "The resulting `tokenized_output` is a sequence of strings, where each string is either an original character or a learned subword from the final BPE vocabulary. This sequence represents the original input text, segmented according to the patterns learned from the training corpus. This detailed breakdown illustrates the core mechanics of BPE, forming the basis for more optimized and complex tokenizers used in modern NLP models."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-multimodaal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
