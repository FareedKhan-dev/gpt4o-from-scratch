{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## End-to-End Text-to-Image Transformer Implementation (Feature Generation)\n",
    "\n",
    "### Introduction: Generating Images from Text via Feature Vectors\n",
    "\n",
    "**Recap: The Multi-Modal Transformer**\n",
    "\n",
    "In `multimodal.ipynb`, we adapted our character-level Transformer to handle both images (via ResNet features) and text prompts for tasks like visual description. We saved the state, including text processing components, tokenizer, and the vision feature projection layer, in `saved_models/multimodal_model.pt`.\n",
    "\n",
    "**The Text-to-Image Challenge (Simplified)**\n",
    "\n",
    "Generating realistic images directly from text is a complex task often involving specialized architectures like Generative Adversarial Networks (GANs) or Diffusion Models. To stay within our inline, Transformer-focused framework, we will tackle a simplified version:\n",
    "\n",
    "*   **Goal:** Given a text prompt (e.g., \"a blue square\"), generate an *image feature vector* that represents the described image.\n",
    "*   **Why Feature Vectors?** Generating raw pixels autoregressively with a basic Transformer is difficult and computationally expensive. Generating a fixed-size feature vector (like those from ResNet) is a more manageable intermediate step.\n",
    "*   **Image Reconstruction:** After generating the feature vector, we will use a simple nearest-neighbor approach on our known training images to visualize the result. The model predicts a feature vector, and we find which of our training images (red square, blue square, green circle) has the most similar feature vector, displaying that image.\n",
    "\n",
    "**Our Approach: Transformer for Feature Prediction**\n",
    "\n",
    "1.  **Load Components:** We load the text Transformer components (embeddings, positional encoding, attention/FFN blocks, final layer norm) and the tokenizer from `multimodal_model.pt`. We also load the frozen ResNet-18 feature extractor to get *target* features during training.\n",
    "2.  **Adapt Architecture:** The input is now only text. We will replace the final output layer (which previously predicted text tokens) with a new linear layer that maps the Transformer's final hidden state to the dimensionality of the ResNet image features (e.g., 512).\n",
    "3.  **Training Data:** We need pairs of (Text Prompt, Target Image). We'll use descriptive prompts for the simple images created previously.\n",
    "4.  **Training Process:** The model reads the prompt, processes it through the Transformer blocks, and uses the new output layer to predict an image feature vector. The loss (MSE) compares this predicted vector to the *actual* feature vector of the target image.\n",
    "5.  **Inference (Generation):** Input a text prompt, get the predicted feature vector from the model, find the closest matching known image feature, and display the corresponding image.\n",
    "\n",
    "**Inline Implementation Style**\n",
    "\n",
    "We continue the extremely detailed, step-by-step, inline implementation with theory, avoiding functions and classes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 0: Setup - Libraries, Loading, Data, Feature Extraction\n",
    "\n",
    "**Goal:** Prepare the environment, load relevant components from the previous multi-modal model, define text-image pair data, and extract target image features."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.1: Import Libraries\n",
    "\n",
    "**Theory:** Import necessary libraries. We need `torch`, `torchvision`, `PIL`, `math`, `os`, `numpy`. We'll also need `scipy.spatial.distance` or `torch.nn.functional.cosine_similarity` later for finding the closest feature vector during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "PyTorch version: 2.6.0+cu118\n",
      "Torchvision version: 0.21.0+cu118\n",
      "Libraries imported.\n",
      "Using device: cuda\n"
     ]
    }
   ],
   "source": [
    "# Import necessary libraries\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.nn import functional as F\n",
    "import torch.optim as optim\n",
    "import torchvision\n",
    "import torchvision.transforms as transforms\n",
    "from PIL import Image, ImageDraw # Added ImageDraw back\n",
    "import math\n",
    "import os\n",
    "import numpy as np\n",
    "# For finding closest vector later\n",
    "from scipy.spatial import distance as scipy_distance \n",
    "\n",
    "# For reproducibility\n",
    "torch.manual_seed(123) # Yet another seed\n",
    "np.random.seed(123)\n",
    "\n",
    "print(f\"PyTorch version: {torch.__version__}\")\n",
    "print(f\"Torchvision version: {torchvision.__version__}\")\n",
    "print(\"Libraries imported.\")\n",
    "\n",
    "# --- Device Configuration ---\n",
    "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.2: Load Relevant State from Multi-Modal Model\n",
    "\n",
    "**Theory:** Load the state dictionary from `multimodal_model.pt`. We need the configuration, the tokenizer, the text Transformer components (embeddings, blocks, etc.), and the ResNet feature extractor (which we'll keep frozen)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0.2: Loading state from multi-modal model...\n",
      "Loaded state dictionary from 'saved_models/multimodal_model.pt'.\n",
      "Extracted model configuration and tokenizer:\n",
      "  vocab_size: 42\n",
      "  d_model: 64\n",
      "  n_layers: 3\n",
      "  n_heads: 4\n",
      "  d_ff: 256\n",
      "  block_size: 64\n",
      "  vision_feature_dim: 512\n",
      "  PAD token ID: 37\n",
      "Loaded positional encoding with shape: torch.Size([1, 64, 64])\n",
      "Stored state dicts for text transformer components.\n",
      "Loading pre-trained vision model (ResNet-18) for target feature extraction...\n",
      "Loaded and froze ResNet-18 feature extractor on device: cuda\n",
      "Defined image transformations.\n"
     ]
    }
   ],
   "source": [
    "# --- Load Saved Multi-Modal Model State ---\n",
    "print(\"\\nStep 0.2: Loading state from multi-modal model...\")\n",
    "model_load_path = 'saved_models/multimodal_model.pt'\n",
    "if not os.path.exists(model_load_path):\n",
    "    raise FileNotFoundError(f\"Error: Model file not found at {model_load_path}. Please ensure 'multimodal.ipynb' was run and saved the model.\")\n",
    "\n",
    "loaded_state_dict = torch.load(model_load_path, map_location=device)\n",
    "print(f\"Loaded state dictionary from '{model_load_path}'.\")\n",
    "\n",
    "# --- Extract Config and Tokenizer ---\n",
    "config = loaded_state_dict['config']\n",
    "vocab_size = config['vocab_size'] # Includes special tokens\n",
    "d_model = config['d_model']\n",
    "n_heads = config['n_heads']\n",
    "n_layers = config['n_layers']\n",
    "d_ff = config['d_ff']\n",
    "# Use block_size from loaded config, might adjust later based only on text prompt length needs\n",
    "block_size = config['block_size']\n",
    "vision_feature_dim = config['vision_feature_dim'] # Dimension of ResNet features (e.g., 512)\n",
    "d_k = d_model // n_heads\n",
    "\n",
    "char_to_int = loaded_state_dict['tokenizer']['char_to_int']\n",
    "int_to_char = loaded_state_dict['tokenizer']['int_to_char']\n",
    "pad_token_id = char_to_int.get('<PAD>', -1) # Get PAD token ID\n",
    "if pad_token_id == -1:\n",
    "    print(\"Warning: PAD token not found in loaded tokenizer!\")\n",
    "\n",
    "print(\"Extracted model configuration and tokenizer:\")\n",
    "print(f\"  vocab_size: {vocab_size}\")\n",
    "print(f\"  d_model: {d_model}\")\n",
    "print(f\"  n_layers: {n_layers}\")\n",
    "print(f\"  n_heads: {n_heads}\")\n",
    "print(f\"  d_ff: {d_ff}\")\n",
    "print(f\"  block_size: {block_size}\")\n",
    "print(f\"  vision_feature_dim: {vision_feature_dim}\")\n",
    "print(f\"  PAD token ID: {pad_token_id}\")\n",
    "\n",
    "# --- Load Positional Encoding --- \n",
    "positional_encoding = loaded_state_dict['positional_encoding'].to(device)\n",
    "# Verify block size consistency\n",
    "if positional_encoding.shape[1] != block_size:\n",
    "    print(f\"Warning: Loaded PE size ({positional_encoding.shape[1]}) doesn't match loaded block_size ({block_size}). Using loaded PE size.\")\n",
    "    # block_size = positional_encoding.shape[1] # Option 1: Use PE's size\n",
    "    # Option 2: Recompute PE (like before) if necessary, but let's try slicing/padding first if needed\n",
    "print(f\"Loaded positional encoding with shape: {positional_encoding.shape}\")\n",
    "\n",
    "# --- Load Text Transformer Components (Weights only, structure created later) ---\n",
    "loaded_embedding_dict = loaded_state_dict['token_embedding_table']\n",
    "loaded_ln1_dicts = loaded_state_dict['layer_norms_1']\n",
    "loaded_qkv_dicts = loaded_state_dict['mha_qkv_linears']\n",
    "loaded_mha_out_dicts = loaded_state_dict['mha_output_linears']\n",
    "loaded_ln2_dicts = loaded_state_dict['layer_norms_2']\n",
    "loaded_ffn1_dicts = loaded_state_dict['ffn_linear_1']\n",
    "loaded_ffn2_dicts = loaded_state_dict['ffn_linear_2']\n",
    "loaded_final_ln_dict = loaded_state_dict['final_layer_norm']\n",
    "print(\"Stored state dicts for text transformer components.\")\n",
    "\n",
    "# --- Load Vision Feature Extractor (ResNet) --- \n",
    "# Theory: Re-load the same ResNet-18 model used before. Keep it frozen.\n",
    "print(\"Loading pre-trained vision model (ResNet-18) for target feature extraction...\")\n",
    "vision_model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "vision_model.fc = nn.Identity() # Remove classifier\n",
    "vision_model = vision_model.to(device)\n",
    "vision_model.eval() # Keep in evaluation mode\n",
    "# Freeze ResNet parameters - VERY IMPORTANT\n",
    "for param in vision_model.parameters():\n",
    "    param.requires_grad = False\n",
    "print(f\"Loaded and froze ResNet-18 feature extractor on device: {device}\")\n",
    "\n",
    "# --- Define Image Transformations (same as before) ---\n",
    "image_transforms = transforms.Compose([\n",
    "    transforms.Resize(256),\n",
    "    transforms.CenterCrop(224),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "print(\"Defined image transformations.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.3: Define Sample Text-to-Image Data\n",
    "\n",
    "**Theory:** Create pairs of (descriptive text prompt, path_to_target_image). Use the same simple images generated in the previous notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0.3: Defining sample text-to-image data...\n",
      "Defined 7 sample text-to-image data points.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 0.3: Defining sample text-to-image data...\")\n",
    "\n",
    "# --- Image Paths (Assuming they exist from multimodal.ipynb) ---\n",
    "sample_data_dir = \"sample_multimodal_data\"\n",
    "image_paths = {\n",
    "    \"red_square\": os.path.join(sample_data_dir, \"red_square.png\"),\n",
    "    \"blue_square\": os.path.join(sample_data_dir, \"blue_square.png\"),\n",
    "    \"green_circle\": os.path.join(sample_data_dir, \"green_circle.png\")\n",
    "}\n",
    "# Verify paths exist\n",
    "for key, path in image_paths.items():\n",
    "    if not os.path.exists(path):\n",
    "        print(f\"Warning: Image file not found at {path}. Attempting to recreate.\")\n",
    "        if key == \"red_square\":\n",
    "            img_ = Image.new('RGB', (64, 64), color = 'red')\n",
    "            img_.save(path)\n",
    "        elif key == \"blue_square\":\n",
    "            img_ = Image.new('RGB', (64, 64), color = 'blue')\n",
    "            img_.save(path)\n",
    "        elif key == \"green_circle\":\n",
    "            img_ = Image.new('RGB', (64, 64), color = 'white')\n",
    "            draw = ImageDraw.Draw(img_)\n",
    "            draw.ellipse((4, 4, 60, 60), fill='green', outline='green')\n",
    "            img_.save(path)\n",
    "        else:\n",
    "             print(f\"Error: Cannot recreate unknown image key '{key}'.\")\n",
    "\n",
    "# --- Define Text Prompt -> Image Path Pairs ---\n",
    "text_to_image_data = [\n",
    "    {\"prompt\": \"a red square\", \"image_path\": image_paths[\"red_square\"]},\n",
    "    {\"prompt\": \"the square is red\", \"image_path\": image_paths[\"red_square\"]},\n",
    "    {\"prompt\": \"show a blue square\", \"image_path\": image_paths[\"blue_square\"]},\n",
    "    {\"prompt\": \"blue shape, square\", \"image_path\": image_paths[\"blue_square\"]},\n",
    "    {\"prompt\": \"a green circle\", \"image_path\": image_paths[\"green_circle\"]},\n",
    "    {\"prompt\": \"the circle, it is green\", \"image_path\": image_paths[\"green_circle\"]},\n",
    "    # Add maybe one more variation\n",
    "    {\"prompt\": \"make a square that is red\", \"image_path\": image_paths[\"red_square\"]}\n",
    "]\n",
    "\n",
    "num_samples = len(text_to_image_data)\n",
    "print(f\"Defined {num_samples} sample text-to-image data points.\")\n",
    "# print(f\"Sample 0: {text_to_image_data[0]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.4: Extract Target Image Features\n",
    "\n",
    "**Theory:** Pre-compute the target feature vectors for all unique images in our dataset using the frozen ResNet. Store these, mapping the image path to its feature tensor. We also store them in a separate list alongside their paths for the nearest-neighbor lookup during generation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0.4: Extracting target image features...\n",
      "Found 3 unique target images to process.\n",
      "  Extracted features for 'blue_square.png', shape: torch.Size([512])\n",
      "  Extracted features for 'green_circle.png', shape: torch.Size([512])\n",
      "  Extracted features for 'red_square.png', shape: torch.Size([512])\n",
      "Finished extracting and storing target image features.\n",
      "Stored 3 known (path, feature) pairs for generation lookup.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 0.4: Extracting target image features...\")\n",
    "target_image_features = {} # Dict: {image_path: feature_tensor}\n",
    "known_features_list = [] # List: [(path, feature_tensor)] for generation lookup\n",
    "\n",
    "# --- Loop Through Unique Image Paths in this dataset ---\n",
    "unique_image_paths_in_data = sorted(list(set(d[\"image_path\"] for d in text_to_image_data)))\n",
    "print(f\"Found {len(unique_image_paths_in_data)} unique target images to process.\")\n",
    "\n",
    "for img_path in unique_image_paths_in_data:\n",
    "    # Avoid re-extracting if already done (e.g., if loading from multimodal)\n",
    "    # if img_path in extracted_image_features: # Check previous notebook's dict\n",
    "    #    feature_vector_squeezed = extracted_image_features[img_path]\n",
    "    #    print(f\"  Using pre-extracted features for '{os.path.basename(img_path)}'\")\n",
    "    # else: \n",
    "    # --- Load Image --- \n",
    "    try:\n",
    "        img = Image.open(img_path).convert('RGB')\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Image file not found at {img_path}. Skipping.\")\n",
    "        continue\n",
    "    \n",
    "    # --- Apply Transformations ---\n",
    "    img_tensor = image_transforms(img).unsqueeze(0).to(device) # (1, 3, 224, 224)\n",
    "\n",
    "    # --- Extract Features (using frozen vision_model) ---\n",
    "    with torch.no_grad():\n",
    "        feature_vector = vision_model(img_tensor) # (1, vision_feature_dim)\n",
    "    feature_vector_squeezed = feature_vector.squeeze(0) # (vision_feature_dim,)\n",
    "    print(f\"  Extracted features for '{os.path.basename(img_path)}', shape: {feature_vector_squeezed.shape}\")\n",
    "\n",
    "    # --- Store Features ---\n",
    "    target_image_features[img_path] = feature_vector_squeezed\n",
    "    known_features_list.append((img_path, feature_vector_squeezed))\n",
    "\n",
    "if not target_image_features:\n",
    "     raise ValueError(\"No target image features were extracted. Cannot proceed.\")\n",
    "\n",
    "print(\"Finished extracting and storing target image features.\")\n",
    "print(f\"Stored {len(known_features_list)} known (path, feature) pairs for generation lookup.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 0.5: Define Training Hyperparameters\n",
    "\n",
    "**Theory:** Adjust hyperparameters for this specific task. We might need a different learning rate or number of epochs. The `block_size` here mainly relates to the maximum prompt length we expect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 0.5: Defining training hyperparameters for text-to-image...\n",
      "Using block_size: 64\n",
      "  Training Params: LR=0.0001, BatchSize=4, Epochs=5000\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 0.5: Defining training hyperparameters for text-to-image...\")\n",
    "\n",
    "# Use block_size from loaded config, ensure it's adequate for prompts\n",
    "max_prompt_len = max(len(d[\"prompt\"]) for d in text_to_image_data)\n",
    "if block_size < max_prompt_len + 1: # +1 for potential special tokens like <EOS> if used in prompt\n",
    "    print(f\"Warning: Loaded block_size ({block_size}) might be small for max prompt length ({max_prompt_len}). Consider increasing block_size if issues arise.\")\n",
    "    # Adjust block_size if needed, and recompute PE / causal mask\n",
    "    # block_size = max_prompt_len + 5 # Example adjustment\n",
    "    # print(f\"Adjusted block_size to {block_size}\")\n",
    "    # Need to recompute PE and causal_mask if block_size changes\n",
    "\n",
    "# Recreate causal mask just in case block_size was adjusted, or use loaded size\n",
    "causal_mask = torch.tril(torch.ones(block_size, block_size, device=device)).view(1, 1, block_size, block_size)\n",
    "print(f\"Using block_size: {block_size}\")\n",
    "\n",
    "learning_rate = 1e-4 # Potentially lower LR for fine-tuning\n",
    "batch_size = 4      # Keep small due to limited data\n",
    "epochs = 5000       # Number of training iterations\n",
    "eval_interval = 500\n",
    "\n",
    "print(f\"  Training Params: LR={learning_rate}, BatchSize={batch_size}, Epochs={epochs}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Model Adaptation and Initialization\n",
    "\n",
    "**Goal:** Rebuild the text Transformer components using the loaded weights and initialize the new output projection layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.1: Initialize Text Transformer Components\n",
    "\n",
    "**Theory:** Create instances of the embedding layer, LayerNorms, and Linear layers for the Transformer blocks, then load the pre-trained weights from the dictionaries stored in Step 0.2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1.1: Initializing Text Transformer components and loading weights...\n",
      "  Loaded Token Embedding Table, shape: torch.Size([42, 64])\n",
      "  Loaded components for 3 Transformer Layers.\n",
      "  Loaded Final LayerNorm.\n",
      "Finished initializing and loading weights for text transformer components.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 1.1: Initializing Text Transformer components and loading weights...\")\n",
    "\n",
    "# --- Token Embedding Table ---\n",
    "token_embedding_table = nn.Embedding(vocab_size, d_model).to(device)\n",
    "token_embedding_table.load_state_dict(loaded_embedding_dict)\n",
    "print(f\"  Loaded Token Embedding Table, shape: {token_embedding_table.weight.shape}\")\n",
    "\n",
    "# --- Transformer Blocks Components ---\n",
    "layer_norms_1 = []\n",
    "mha_qkv_linears = []\n",
    "mha_output_linears = []\n",
    "layer_norms_2 = []\n",
    "ffn_linear_1 = []\n",
    "ffn_linear_2 = []\n",
    "\n",
    "for i in range(n_layers):\n",
    "    # LayerNorm 1\n",
    "    ln1 = nn.LayerNorm(d_model).to(device)\n",
    "    ln1.load_state_dict(loaded_ln1_dicts[i])\n",
    "    layer_norms_1.append(ln1)\n",
    "\n",
    "    # MHA QKV Linear (Check bias presence)\n",
    "    qkv_dict = loaded_qkv_dicts[i]\n",
    "    has_bias = 'bias' in qkv_dict\n",
    "    qkv_linear = nn.Linear(d_model, 3 * d_model, bias=has_bias).to(device)\n",
    "    qkv_linear.load_state_dict(qkv_dict)\n",
    "    mha_qkv_linears.append(qkv_linear)\n",
    "\n",
    "    # MHA Output Linear (Check bias presence)\n",
    "    mha_out_dict = loaded_mha_out_dicts[i]\n",
    "    has_bias = 'bias' in mha_out_dict\n",
    "    output_linear_mha = nn.Linear(d_model, d_model, bias=has_bias).to(device)\n",
    "    output_linear_mha.load_state_dict(mha_out_dict)\n",
    "    mha_output_linears.append(output_linear_mha)\n",
    "\n",
    "    # LayerNorm 2\n",
    "    ln2 = nn.LayerNorm(d_model).to(device)\n",
    "    ln2.load_state_dict(loaded_ln2_dicts[i])\n",
    "    layer_norms_2.append(ln2)\n",
    "\n",
    "    # FFN Linear 1 (Check bias presence)\n",
    "    ffn1_dict = loaded_ffn1_dicts[i]\n",
    "    has_bias = 'bias' in ffn1_dict\n",
    "    lin1 = nn.Linear(d_model, d_ff, bias=has_bias).to(device)\n",
    "    lin1.load_state_dict(ffn1_dict)\n",
    "    ffn_linear_1.append(lin1)\n",
    "\n",
    "    # FFN Linear 2 (Check bias presence)\n",
    "    ffn2_dict = loaded_ffn2_dicts[i]\n",
    "    has_bias = 'bias' in ffn2_dict\n",
    "    lin2 = nn.Linear(d_ff, d_model, bias=has_bias).to(device)\n",
    "    lin2.load_state_dict(ffn2_dict)\n",
    "    ffn_linear_2.append(lin2)\n",
    "\n",
    "print(f\"  Loaded components for {n_layers} Transformer Layers.\")\n",
    "\n",
    "# --- Final LayerNorm ---\n",
    "final_layer_norm = nn.LayerNorm(d_model).to(device)\n",
    "final_layer_norm.load_state_dict(loaded_final_ln_dict)\n",
    "print(\"  Loaded Final LayerNorm.\")\n",
    "\n",
    "print(\"Finished initializing and loading weights for text transformer components.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 1.2: Initialize New Output Projection Layer\n",
    "\n",
    "**Theory:** Create the new linear layer that will map the final hidden state of the Transformer (`d_model`) to the dimension of the image feature vector (`vision_feature_dim`). This layer's weights are initialized randomly and will be trained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 1.2: Initializing new output projection layer (Text -> Image Feature)...\n",
      "  Initialized Text-to-Image-Feature Output Layer: 64 -> 512. Device: cuda\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 1.2: Initializing new output projection layer (Text -> Image Feature)...\")\n",
    "\n",
    "text_to_image_feature_layer = nn.Linear(d_model, vision_feature_dim).to(device)\n",
    "\n",
    "print(f\"  Initialized Text-to-Image-Feature Output Layer: {d_model} -> {vision_feature_dim}. Device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 2: Data Preparation for Text-to-Image Training\n",
    "\n",
    "**Goal:** Tokenize and pad the text prompts, and pair them with their corresponding target image feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.1: Tokenize and Pad Prompts\n",
    "\n",
    "**Theory:** Convert text prompts into sequences of token IDs using `char_to_int`. Pad each sequence to `block_size` using `pad_token_id`. Also create corresponding attention masks (1 for real tokens, 0 for padding)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2.1: Tokenizing and padding text prompts...\n",
      "Created 7 padded prompt sequences and gathered target features.\n",
      "  Prompt Input IDs shape: torch.Size([7, 64])\n",
      "  Prompt Attention Mask shape: torch.Size([7, 64])\n",
      "  Target Features shape: torch.Size([7, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 2.1: Tokenizing and padding text prompts...\")\n",
    "\n",
    "prepared_prompts = []\n",
    "target_features_ordered = [] # Store target features in the same order\n",
    "\n",
    "for sample in text_to_image_data:\n",
    "    prompt = sample[\"prompt\"]\n",
    "    image_path = sample[\"image_path\"]\n",
    "\n",
    "    # --- Tokenize Prompt --- \n",
    "    prompt_ids_no_pad = [char_to_int[ch] for ch in prompt]\n",
    "\n",
    "    # --- Padding --- \n",
    "    current_len = len(prompt_ids_no_pad)\n",
    "    pad_len = block_size - current_len\n",
    "\n",
    "    if pad_len < 0:\n",
    "        print(f\"Warning: Prompt length ({current_len}) exceeds block_size ({block_size}). Truncating prompt.\")\n",
    "        prompt_ids = prompt_ids_no_pad[:block_size]\n",
    "        pad_len = 0\n",
    "        current_len = block_size\n",
    "    else:\n",
    "        prompt_ids = prompt_ids_no_pad + ([pad_token_id] * pad_len)\n",
    "\n",
    "    # --- Create Attention Mask --- \n",
    "    attention_mask = ([1] * current_len) + ([0] * pad_len)\n",
    "\n",
    "    # --- Store Prompt Data --- \n",
    "    prepared_prompts.append({\n",
    "        \"input_ids\": torch.tensor(prompt_ids, dtype=torch.long),\n",
    "        \"attention_mask\": torch.tensor(attention_mask, dtype=torch.long)\n",
    "    })\n",
    "\n",
    "    # --- Store Corresponding Target Feature --- \n",
    "    if image_path in target_image_features:\n",
    "        target_features_ordered.append(target_image_features[image_path])\n",
    "    else:\n",
    "        print(f\"Error: Target feature not found for {image_path}. Data mismatch?\")\n",
    "        # Handle error - maybe skip this sample or raise exception\n",
    "        target_features_ordered.append(torch.zeros(vision_feature_dim, device=device)) # Placeholder\n",
    "\n",
    "# --- Stack into Tensors --- \n",
    "all_prompt_input_ids = torch.stack([p['input_ids'] for p in prepared_prompts])\n",
    "all_prompt_attention_masks = torch.stack([p['attention_mask'] for p in prepared_prompts])\n",
    "all_target_features = torch.stack(target_features_ordered)\n",
    "\n",
    "num_sequences_available = all_prompt_input_ids.shape[0]\n",
    "print(f\"Created {num_sequences_available} padded prompt sequences and gathered target features.\")\n",
    "print(f\"  Prompt Input IDs shape: {all_prompt_input_ids.shape}\") # (num_samples, block_size)\n",
    "print(f\"  Prompt Attention Mask shape: {all_prompt_attention_masks.shape}\") # (num_samples, block_size)\n",
    "print(f\"  Target Features shape: {all_target_features.shape}\") # (num_samples, vision_feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2.2: Batching Strategy (Random Sampling)\n",
    "\n",
    "**Theory:** Set up for random batch sampling during training. We'll select random indices and grab the corresponding prompt IDs, masks, and target image features."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 2.2: Preparing for batching text-to-image data...\n",
      "Data ready for training. Will sample batches of size 4 randomly.\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 2.2: Preparing for batching text-to-image data...\")\n",
    "\n",
    "# Check batch size feasibility\n",
    "if num_sequences_available < batch_size:\n",
    "    print(f\"Warning: Number of sequences ({num_sequences_available}) is less than batch size ({batch_size}). Adjusting batch size.\")\n",
    "    batch_size = num_sequences_available\n",
    "\n",
    "print(f\"Data ready for training. Will sample batches of size {batch_size} randomly.\")\n",
    "# In the training loop, we will use random indices to get:\n",
    "# xb_prompt_ids = all_prompt_input_ids[indices]\n",
    "# batch_prompt_masks = all_prompt_attention_masks[indices]\n",
    "# yb_target_features = all_target_features[indices]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 3: Text-to-Image Training Loop (Inline)\n",
    "\n",
    "**Goal:** Train the model to map text prompts to image feature vectors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.1: Define Optimizer and Loss Function\n",
    "\n",
    "**Theory:** Gather *trainable* parameters (Transformer components + new output layer). Define the optimizer (AdamW). Define the loss function - Mean Squared Error (MSE) is suitable for comparing the predicted and target feature vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3.1: Defining Optimizer and Loss Function for text-to-image...\n",
      "  Optimizer defined: AdamW with lr=0.0001\n",
      "  Managing 38 parameter groups/tensors.\n",
      "  Loss function defined: MSELoss\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 3.1: Defining Optimizer and Loss Function for text-to-image...\")\n",
    "\n",
    "# --- Gather Trainable Parameters --- \n",
    "# Includes Transformer components and the new output layer\n",
    "all_trainable_parameters_t2i = list(token_embedding_table.parameters())\n",
    "for i in range(n_layers):\n",
    "    all_trainable_parameters_t2i.extend(list(layer_norms_1[i].parameters()))\n",
    "    all_trainable_parameters_t2i.extend(list(mha_qkv_linears[i].parameters()))\n",
    "    all_trainable_parameters_t2i.extend(list(mha_output_linears[i].parameters()))\n",
    "    all_trainable_parameters_t2i.extend(list(layer_norms_2[i].parameters()))\n",
    "    all_trainable_parameters_t2i.extend(list(ffn_linear_1[i].parameters()))\n",
    "    all_trainable_parameters_t2i.extend(list(ffn_linear_2[i].parameters()))\n",
    "all_trainable_parameters_t2i.extend(list(final_layer_norm.parameters()))\n",
    "all_trainable_parameters_t2i.extend(list(text_to_image_feature_layer.parameters())) # Add the new layer\n",
    "\n",
    "# --- Define Optimizer --- \n",
    "optimizer = optim.AdamW(all_trainable_parameters_t2i, lr=learning_rate)\n",
    "print(f\"  Optimizer defined: AdamW with lr={learning_rate}\")\n",
    "print(f\"  Managing {len(all_trainable_parameters_t2i)} parameter groups/tensors.\")\n",
    "\n",
    "# --- Define Loss Function --- \n",
    "# Theory: MSE loss compares the predicted feature vector to the target feature vector.\n",
    "criterion = nn.MSELoss()\n",
    "print(f\"  Loss function defined: {type(criterion).__name__}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3.2: The Training Loop\n",
    "\n",
    "**Theory:** Iterate for `epochs`. In each step:\n",
    "1.  Select batch (prompt IDs, masks, target features).\n",
    "2.  Perform forward pass: Embed prompt, add PE, pass through Transformer blocks, apply final LN, project to image feature dimension using `text_to_image_feature_layer`.\n",
    "3.  Calculate MSE loss between predicted and target feature vectors.\n",
    "4.  Backpropagate and update weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 3.2: Starting Text-to-Image Training Loop...\n",
      "  Epoch 1/5000, MSE Loss: 0.880764\n",
      "  Epoch 501/5000, MSE Loss: 0.022411\n",
      "  Epoch 1001/5000, MSE Loss: 0.006695\n",
      "  Epoch 1501/5000, MSE Loss: 0.000073\n",
      "  Epoch 2001/5000, MSE Loss: 0.000015\n",
      "  Epoch 2501/5000, MSE Loss: 0.000073\n",
      "  Epoch 3001/5000, MSE Loss: 0.000002\n",
      "  Epoch 3501/5000, MSE Loss: 0.000011\n",
      "  Epoch 4001/5000, MSE Loss: 0.000143\n",
      "  Epoch 4501/5000, MSE Loss: 0.000013\n",
      "  Epoch 5000/5000, MSE Loss: 0.000030\n",
      "--- Text-to-Image Training Loop Completed ---\n",
      "\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 2000x300 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nStep 3.2: Starting Text-to-Image Training Loop...\")\n",
    "\n",
    "t2i_losses = []\n",
    "\n",
    "# --- Set Trainable Layers to Training Mode ---\n",
    "token_embedding_table.train()\n",
    "for i in range(n_layers):\n",
    "    layer_norms_1[i].train()\n",
    "    mha_qkv_linears[i].train()\n",
    "    mha_output_linears[i].train()\n",
    "    layer_norms_2[i].train()\n",
    "    ffn_linear_1[i].train()\n",
    "    ffn_linear_2[i].train()\n",
    "final_layer_norm.train()\n",
    "text_to_image_feature_layer.train() # New layer also needs training mode\n",
    "# vision_model remains in eval() mode\n",
    "\n",
    "# --- Training Loop ---\n",
    "for epoch in range(epochs):\n",
    "\n",
    "    # --- 1. Batch Selection --- \n",
    "    indices = torch.randint(0, num_sequences_available, (batch_size,))\n",
    "    xb_prompt_ids = all_prompt_input_ids[indices].to(device)      # (B, T)\n",
    "    batch_prompt_masks = all_prompt_attention_masks[indices].to(device) # (B, T)\n",
    "    yb_target_features = all_target_features[indices].to(device) # (B, vision_feature_dim)\n",
    "\n",
    "    # --- 2. Forward Pass --- \n",
    "    B, T = xb_prompt_ids.shape\n",
    "    C = d_model\n",
    "\n",
    "    # --- Embeddings + PE ---\n",
    "    token_embed = token_embedding_table(xb_prompt_ids) # (B, T, C)\n",
    "    pos_enc_slice = positional_encoding[:, :T, :]    # (1, T, C)\n",
    "    x = token_embed + pos_enc_slice                # (B, T, C)\n",
    "\n",
    "    # --- Transformer Blocks --- \n",
    "    # Create attention mask (Causal + Padding for prompts)\n",
    "    padding_mask_expanded = batch_prompt_masks.unsqueeze(1).unsqueeze(2) # (B, 1, 1, T)\n",
    "    combined_attn_mask = causal_mask[:,:,:T,:T] * padding_mask_expanded # (B, 1, T, T)\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        x_input_block = x\n",
    "        # Pre-LN MHA\n",
    "        x_ln1 = layer_norms_1[i](x_input_block)\n",
    "        qkv = mha_qkv_linears[i](x_ln1)\n",
    "        qkv = qkv.view(B, T, n_heads, 3 * d_k).permute(0, 2, 1, 3)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "        attn_scores = (q @ k.transpose(-2, -1)) * (d_k ** -0.5)\n",
    "        attn_scores_masked = attn_scores.masked_fill(combined_attn_mask == 0, float('-inf'))\n",
    "        attention_weights = F.softmax(attn_scores_masked, dim=-1)\n",
    "        attention_weights = torch.nan_to_num(attention_weights)\n",
    "        attn_output = attention_weights @ v\n",
    "        attn_output = attn_output.permute(0, 2, 1, 3).contiguous().view(B, T, C)\n",
    "        mha_result = mha_output_linears[i](attn_output)\n",
    "        x = x_input_block + mha_result # Residual 1\n",
    "        # Pre-LN FFN\n",
    "        x_input_ffn = x\n",
    "        x_ln2 = layer_norms_2[i](x_input_ffn)\n",
    "        ffn_hidden = ffn_linear_1[i](x_ln2)\n",
    "        ffn_activated = F.relu(ffn_hidden)\n",
    "        ffn_output = ffn_linear_2[i](ffn_activated)\n",
    "        x = x_input_ffn + ffn_output # Residual 2\n",
    "\n",
    "    # --- Final LayerNorm --- \n",
    "    final_norm_output = final_layer_norm(x) # (B, T, C)\n",
    "\n",
    "    # --- Select Hidden State for Prediction --- \n",
    "    # Theory: We need one vector per sequence to predict the image feature vector.\n",
    "    # We can take the hidden state of the *last non-padding* token.\n",
    "    # Find indices of the last non-padding token for each sequence in the batch.\n",
    "    # batch_prompt_masks is (B, T), value is 1 for non-pad, 0 for pad.\n",
    "    # `torch.sum(mask, 1) - 1` gives the index of the last '1'.\n",
    "    last_token_indices = torch.sum(batch_prompt_masks, 1) - 1 # Shape: (B,)\n",
    "    # Ensure indices are within bounds (handle case of all padding, though unlikely)\n",
    "    last_token_indices = torch.clamp(last_token_indices, min=0)\n",
    "    # Gather the hidden states corresponding to these last tokens.\n",
    "    # We need to index final_norm_output[batch_index, token_index, :]\n",
    "    batch_indices = torch.arange(B, device=device)\n",
    "    last_token_hidden_states = final_norm_output[batch_indices, last_token_indices, :] # (B, C)\n",
    "\n",
    "    # --- Project to Image Feature Dimension --- \n",
    "    # Theory: Use the new output layer to predict the image feature vector.\n",
    "    predicted_image_features = text_to_image_feature_layer(last_token_hidden_states) # (B, vision_feature_dim)\n",
    "\n",
    "    # --- 3. Calculate Loss --- \n",
    "    # Theory: Compute MSE between predicted features and target features.\n",
    "    loss = criterion(predicted_image_features, yb_target_features)\n",
    "\n",
    "    # --- 4. Zero Gradients --- \n",
    "    optimizer.zero_grad()\n",
    "\n",
    "    # --- 5. Backward Pass --- \n",
    "    if not torch.isnan(loss) and not torch.isinf(loss):\n",
    "        loss.backward()\n",
    "        # Optional: Gradient Clipping\n",
    "        # torch.nn.utils.clip_grad_norm_(all_trainable_parameters_t2i, max_norm=1.0)\n",
    "        \n",
    "        # --- 6. Update Parameters --- \n",
    "        optimizer.step()\n",
    "    else:\n",
    "        print(f\"Warning: Invalid loss detected (NaN or Inf) at epoch {epoch+1}. Skipping optimizer step.\")\n",
    "        loss = None\n",
    "\n",
    "    # --- Logging --- \n",
    "    if loss is not None:\n",
    "        current_loss = loss.item()\n",
    "        t2i_losses.append(current_loss)\n",
    "        if epoch % eval_interval == 0 or epoch == epochs - 1:\n",
    "            print(f\"  Epoch {epoch+1}/{epochs}, MSE Loss: {current_loss:.6f}\")\n",
    "    elif epoch % eval_interval == 0 or epoch == epochs - 1:\n",
    "        print(f\"  Epoch {epoch+1}/{epochs}, Loss: Invalid (NaN/Inf)\")\n",
    "\n",
    "print(\"--- Text-to-Image Training Loop Completed ---\\n\")\n",
    "\n",
    "# Optional: Plot losses\n",
    "import matplotlib.pyplot as plt\n",
    "plt.figure(figsize=(20, 3))\n",
    "plt.plot(t2i_losses)\n",
    "plt.title(\"Text-to-Image Training Loss (MSE)\")\n",
    "plt.xlabel(\"Epoch\")\n",
    "plt.ylabel(\"MSE Loss\")\n",
    "plt.grid(True)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 4: Text-to-Image Generation (Inline)\n",
    "\n",
    "**Goal:** Use the trained model to generate an image feature vector from a text prompt and find the closest matching known image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.1: Prepare Input Prompt\n",
    "\n",
    "**Theory:** Define a new text prompt, tokenize it, and pad it to `block_size`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4.1: Preparing input prompt for generation...\n",
      "Input Prompt: 'a blue square shape'\n",
      "Prepared prompt tensor shape: torch.Size([1, 64])\n",
      "Prepared mask tensor shape: torch.Size([1, 64])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 4.1: Preparing input prompt for generation...\")\n",
    "\n",
    "# --- Input Prompt --- \n",
    "generation_prompt_text = \"a blue square shape\"\n",
    "print(f\"Input Prompt: '{generation_prompt_text}'\")\n",
    "\n",
    "# --- Tokenize and Pad --- \n",
    "gen_prompt_ids_no_pad = [char_to_int.get(ch, pad_token_id) for ch in generation_prompt_text] # Use get with default for safety\n",
    "gen_current_len = len(gen_prompt_ids_no_pad)\n",
    "gen_pad_len = block_size - gen_current_len\n",
    "\n",
    "if gen_pad_len < 0:\n",
    "    print(f\"Warning: Generation prompt length ({gen_current_len}) exceeds block_size ({block_size}). Truncating.\")\n",
    "    gen_prompt_ids = gen_prompt_ids_no_pad[:block_size]\n",
    "    gen_pad_len = 0\n",
    "    gen_current_len = block_size\n",
    "else:\n",
    "    gen_prompt_ids = gen_prompt_ids_no_pad + ([pad_token_id] * gen_pad_len)\n",
    "\n",
    "# --- Create Attention Mask --- \n",
    "gen_attention_mask = ([1] * gen_current_len) + ([0] * gen_pad_len)\n",
    "\n",
    "# --- Convert to Tensor --- \n",
    "xb_gen_prompt_ids = torch.tensor([gen_prompt_ids], dtype=torch.long, device=device) # Add batch dim B=1\n",
    "batch_gen_prompt_masks = torch.tensor([gen_attention_mask], dtype=torch.long, device=device) # Add batch dim B=1\n",
    "\n",
    "print(f\"Prepared prompt tensor shape: {xb_gen_prompt_ids.shape}\")\n",
    "print(f\"Prepared mask tensor shape: {batch_gen_prompt_masks.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.2: Generate Image Feature Vector\n",
    "\n",
    "**Theory:** Perform a forward pass with the input prompt using the trained model (in evaluation mode) to get the predicted image feature vector."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4.2: Generating image feature vector...\n",
      "Generated predicted feature vector with shape: torch.Size([1, 512])\n"
     ]
    }
   ],
   "source": [
    "print(\"\\nStep 4.2: Generating image feature vector...\")\n",
    "\n",
    "# --- Set Model to Evaluation Mode --- \n",
    "token_embedding_table.eval()\n",
    "for i in range(n_layers):\n",
    "    layer_norms_1[i].eval()\n",
    "    mha_qkv_linears[i].eval()\n",
    "    mha_output_linears[i].eval()\n",
    "    layer_norms_2[i].eval()\n",
    "    ffn_linear_1[i].eval()\n",
    "    ffn_linear_2[i].eval()\n",
    "final_layer_norm.eval()\n",
    "text_to_image_feature_layer.eval()\n",
    "\n",
    "# --- Forward Pass --- \n",
    "with torch.no_grad():\n",
    "    B_gen, T_gen = xb_gen_prompt_ids.shape\n",
    "    C_gen = d_model\n",
    "\n",
    "    # Embeddings + PE\n",
    "    token_embed_gen = token_embedding_table(xb_gen_prompt_ids)\n",
    "    pos_enc_slice_gen = positional_encoding[:, :T_gen, :]\n",
    "    x_gen = token_embed_gen + pos_enc_slice_gen\n",
    "\n",
    "    # Transformer Blocks\n",
    "    padding_mask_expanded_gen = batch_gen_prompt_masks.unsqueeze(1).unsqueeze(2)\n",
    "    combined_attn_mask_gen = causal_mask[:,:,:T_gen,:T_gen] * padding_mask_expanded_gen\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        x_input_block_gen = x_gen\n",
    "        # Pre-LN MHA\n",
    "        x_ln1_gen = layer_norms_1[i](x_input_block_gen)\n",
    "        qkv_gen = mha_qkv_linears[i](x_ln1_gen)\n",
    "        qkv_gen = qkv_gen.view(B_gen, T_gen, n_heads, 3 * d_k).permute(0, 2, 1, 3)\n",
    "        q_gen, k_gen, v_gen = qkv_gen.chunk(3, dim=-1)\n",
    "        attn_scores_gen = (q_gen @ k_gen.transpose(-2, -1)) * (d_k ** -0.5)\n",
    "        attn_scores_masked_gen = attn_scores_gen.masked_fill(combined_attn_mask_gen == 0, float('-inf'))\n",
    "        attention_weights_gen = F.softmax(attn_scores_masked_gen, dim=-1)\n",
    "        attention_weights_gen = torch.nan_to_num(attention_weights_gen)\n",
    "        attn_output_gen = attention_weights_gen @ v_gen\n",
    "        attn_output_gen = attn_output_gen.permute(0, 2, 1, 3).contiguous().view(B_gen, T_gen, C_gen)\n",
    "        mha_result_gen = mha_output_linears[i](attn_output_gen)\n",
    "        x_gen = x_input_block_gen + mha_result_gen # Residual 1\n",
    "        # Pre-LN FFN\n",
    "        x_input_ffn_gen = x_gen\n",
    "        x_ln2_gen = layer_norms_2[i](x_input_ffn_gen)\n",
    "        ffn_hidden_gen = ffn_linear_1[i](x_ln2_gen)\n",
    "        ffn_activated_gen = F.relu(ffn_hidden_gen)\n",
    "        ffn_output_gen = ffn_linear_2[i](ffn_activated_gen)\n",
    "        x_gen = x_input_ffn_gen + ffn_output_gen # Residual 2\n",
    "        \n",
    "    # Final LayerNorm\n",
    "    final_norm_output_gen = final_layer_norm(x_gen)\n",
    "\n",
    "    # Select Hidden State (use last non-padding token's state)\n",
    "    last_token_indices_gen = torch.sum(batch_gen_prompt_masks, 1) - 1\n",
    "    last_token_indices_gen = torch.clamp(last_token_indices_gen, min=0)\n",
    "    batch_indices_gen = torch.arange(B_gen, device=device)\n",
    "    last_token_hidden_states_gen = final_norm_output_gen[batch_indices_gen, last_token_indices_gen, :]\n",
    "\n",
    "    # Project to Image Feature Dimension\n",
    "    predicted_feature_vector = text_to_image_feature_layer(last_token_hidden_states_gen)\n",
    "\n",
    "print(f\"Generated predicted feature vector with shape: {predicted_feature_vector.shape}\") # Should be (1, vision_feature_dim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 4.3: Find Closest Known Image (Simplified Reconstruction)\n",
    "\n",
    "**Theory:** Compare the predicted feature vector to the pre-computed feature vectors of our known training images (`known_features_list`). Find the known image whose feature vector has the smallest distance (e.g., Euclidean or Cosine distance) to the predicted vector. Display that known image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Step 4.3: Finding closest known image...\n",
      "Closest match found: 'blue_square.png' with distance 4.8436\n",
      "Displaying the closest matching image:\n"
     ]
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 640x480 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "print(\"\\nStep 4.3: Finding closest known image...\")\n",
    "\n",
    "# --- Calculate Distances --- \n",
    "# Theory: Calculate the distance between the predicted vector and each known vector.\n",
    "# We use Cosine distance: 1 - cosine_similarity. Smaller distance is better.\n",
    "predicted_vec = predicted_feature_vector.squeeze(0).cpu().numpy() # Move to CPU and convert to numpy for scipy\n",
    "\n",
    "min_distance = float('inf')\n",
    "closest_image_path = None\n",
    "\n",
    "for known_path, known_vec_tensor in known_features_list:\n",
    "    known_vec = known_vec_tensor.cpu().numpy()\n",
    "    # Calculate cosine distance\n",
    "    # dist = scipy_distance.cosine(predicted_vec, known_vec)\n",
    "    # Or Calculate Euclidean distance (L2 norm)\n",
    "    dist = scipy_distance.euclidean(predicted_vec, known_vec)\n",
    "\n",
    "    # print(f\"  Distance to {os.path.basename(known_path)}: {dist:.4f}\") # Optional: print distances\n",
    "\n",
    "    if dist < min_distance:\n",
    "        min_distance = dist\n",
    "        closest_image_path = known_path\n",
    "\n",
    "# --- Display Result --- \n",
    "if closest_image_path:\n",
    "    print(f\"Closest match found: '{os.path.basename(closest_image_path)}' with distance {min_distance:.4f}\")\n",
    "    # Display the image using PIL or matplotlib\n",
    "    try:\n",
    "        matched_img = Image.open(closest_image_path)\n",
    "        print(\"Displaying the closest matching image:\")\n",
    "        # In a notebook environment, simply displaying the object often works:\n",
    "        # matched_img \n",
    "        # Or use matplotlib:\n",
    "        import matplotlib.pyplot as plt\n",
    "        plt.imshow(matched_img)\n",
    "        plt.title(f\"Generated for: '{generation_prompt_text}'\\nClosest Match: {os.path.basename(closest_image_path)}\")\n",
    "        plt.axis('off')\n",
    "        plt.show()\n",
    "    except FileNotFoundError:\n",
    "        print(f\"Error: Could not load the matched image file at {closest_image_path}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error displaying image: {e}\")\n",
    "else:\n",
    "    print(\"Could not determine the closest image.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 5: Save the model state (optional)\n",
    "\n",
    "To save our text to image generation model, you need to create a dictionary with all model components and configurations, then use torch.save(). Here's how to do it:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Text-to-Image model saved to saved_models\\text_to_image_model.pt\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "save_dir = 'saved_models'\n",
    "os.makedirs(save_dir, exist_ok=True)\n",
    "save_path = os.path.join(save_dir, 'text_to_image_model.pt')\n",
    "\n",
    "# Create a dictionary with all relevant components and configurations\n",
    "text_to_image_state_dict = {\n",
    "    # Configuration\n",
    "    'config': {\n",
    "        'vocab_size': vocab_size,\n",
    "        'd_model': d_model,\n",
    "        'n_heads': n_heads,\n",
    "        'n_layers': n_layers,\n",
    "        'd_ff': d_ff,\n",
    "        'block_size': block_size,\n",
    "        'vision_feature_dim': vision_feature_dim\n",
    "    },\n",
    "    # Tokenizer\n",
    "    'tokenizer': {\n",
    "        'char_to_int': char_to_int,\n",
    "        'int_to_char': int_to_char\n",
    "    },\n",
    "    # Model weights (trainable parts)\n",
    "    'token_embedding_table': token_embedding_table.state_dict(),\n",
    "    'positional_encoding': positional_encoding, # Not trained, but needed for reconstruction\n",
    "    'layer_norms_1': [ln.state_dict() for ln in layer_norms_1],\n",
    "    'mha_qkv_linears': [l.state_dict() for l in mha_qkv_linears],\n",
    "    'mha_output_linears': [l.state_dict() for l in mha_output_linears],\n",
    "    'layer_norms_2': [ln.state_dict() for ln in layer_norms_2],\n",
    "    'ffn_linear_1': [l.state_dict() for l in ffn_linear_1],\n",
    "    'ffn_linear_2': [l.state_dict() for l in ffn_linear_2],\n",
    "    'final_layer_norm': final_layer_norm.state_dict(),\n",
    "    'text_to_image_feature_layer': text_to_image_feature_layer.state_dict() # The new output layer\n",
    "    # Note: We don't save the frozen vision_model weights here, \n",
    "    # as we assume it's loaded separately from torchvision during use.\n",
    "}\n",
    "\n",
    "# Save to file\n",
    "torch.save(text_to_image_state_dict, save_path)\n",
    "print(f\"Text-to-Image model saved to {save_path}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Loading the Saved Text-to-Image Model\n",
    "\n",
    "To load the model back, you would reverse the saving process:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded state dictionary from 'saved_models/text_to_image_model.pt'.\n",
      "Text-to-Image model components loaded successfully.\n"
     ]
    }
   ],
   "source": [
    "# Load the saved state dictionary\n",
    "load_path = 'saved_models/text_to_image_model.pt'\n",
    "if os.path.exists(load_path):\n",
    "    loaded_t2i_state = torch.load(load_path, map_location=device)\n",
    "    print(f\"Loaded state dictionary from '{load_path}'.\")\n",
    "\n",
    "    # Extract configuration and tokenizer\n",
    "    config = loaded_t2i_state['config']\n",
    "    vocab_size = config['vocab_size']\n",
    "    d_model = config['d_model']\n",
    "    n_heads = config['n_heads']\n",
    "    n_layers = config['n_layers']\n",
    "    d_ff = config['d_ff']\n",
    "    block_size = config['block_size']\n",
    "    vision_feature_dim = config['vision_feature_dim']\n",
    "    d_k = d_model // n_heads\n",
    "\n",
    "    char_to_int = loaded_t2i_state['tokenizer']['char_to_int']\n",
    "    int_to_char = loaded_t2i_state['tokenizer']['int_to_char']\n",
    "\n",
    "    # Recreate causal mask\n",
    "    causal_mask = torch.tril(torch.ones(block_size, block_size, device=device)).view(1, 1, block_size, block_size)\n",
    "\n",
    "    # Rebuild and load model components\n",
    "    token_embedding_table = nn.Embedding(vocab_size, d_model).to(device)\n",
    "    token_embedding_table.load_state_dict(loaded_t2i_state['token_embedding_table'])\n",
    "\n",
    "    positional_encoding = loaded_t2i_state['positional_encoding'].to(device)\n",
    "\n",
    "    layer_norms_1 = []\n",
    "    mha_qkv_linears = []\n",
    "    mha_output_linears = []\n",
    "    layer_norms_2 = []\n",
    "    ffn_linear_1 = []\n",
    "    ffn_linear_2 = []\n",
    "\n",
    "    for i in range(n_layers):\n",
    "        # Load Layer Norm 1\n",
    "        ln1 = nn.LayerNorm(d_model).to(device)\n",
    "        ln1.load_state_dict(loaded_t2i_state['layer_norms_1'][i])\n",
    "        layer_norms_1.append(ln1)\n",
    "        \n",
    "        # Load MHA QKV Linear\n",
    "        qkv_dict = loaded_t2i_state['mha_qkv_linears'][i]\n",
    "        has_bias = 'bias' in qkv_dict\n",
    "        qkv = nn.Linear(d_model, 3 * d_model, bias=has_bias).to(device)\n",
    "        qkv.load_state_dict(qkv_dict)\n",
    "        mha_qkv_linears.append(qkv)\n",
    "\n",
    "        # Load MHA Output Linear\n",
    "        mha_out_dict = loaded_t2i_state['mha_output_linears'][i]\n",
    "        has_bias = 'bias' in mha_out_dict\n",
    "        mha_out = nn.Linear(d_model, d_model, bias=has_bias).to(device)\n",
    "        mha_out.load_state_dict(mha_out_dict)\n",
    "        mha_output_linears.append(mha_out)\n",
    "\n",
    "        # Load Layer Norm 2\n",
    "        ln2 = nn.LayerNorm(d_model).to(device)\n",
    "        ln2.load_state_dict(loaded_t2i_state['layer_norms_2'][i])\n",
    "        layer_norms_2.append(ln2)\n",
    "\n",
    "        # Load FFN Linear 1\n",
    "        ffn1_dict = loaded_t2i_state['ffn_linear_1'][i]\n",
    "        has_bias = 'bias' in ffn1_dict\n",
    "        ff1 = nn.Linear(d_model, d_ff, bias=has_bias).to(device)\n",
    "        ff1.load_state_dict(ffn1_dict)\n",
    "        ffn_linear_1.append(ff1)\n",
    "\n",
    "        # Load FFN Linear 2\n",
    "        ffn2_dict = loaded_t2i_state['ffn_linear_2'][i]\n",
    "        has_bias = 'bias' in ffn2_dict\n",
    "        ff2 = nn.Linear(d_ff, d_model, bias=has_bias).to(device)\n",
    "        ff2.load_state_dict(ffn2_dict)\n",
    "        ffn_linear_2.append(ff2)\n",
    "\n",
    "    # Load Final LayerNorm\n",
    "    final_layer_norm = nn.LayerNorm(d_model).to(device)\n",
    "    final_layer_norm.load_state_dict(loaded_t2i_state['final_layer_norm'])\n",
    "\n",
    "    # Load Text-to-Image Feature Layer\n",
    "    t2i_out_dict = loaded_t2i_state['text_to_image_feature_layer']\n",
    "    has_bias = 'bias' in t2i_out_dict\n",
    "    text_to_image_feature_layer = nn.Linear(d_model, vision_feature_dim, bias=has_bias).to(device)\n",
    "    text_to_image_feature_layer.load_state_dict(t2i_out_dict)\n",
    "    \n",
    "    print(\"Text-to-Image model components loaded successfully.\")\n",
    "\n",
    "    # Remember to also load the vision_model separately if needed for targets/comparison\n",
    "    # vision_model = torchvision.models.resnet18(weights=torchvision.models.ResNet18_Weights.DEFAULT)\n",
    "    # vision_model.fc = nn.Identity()\n",
    "    # vision_model = vision_model.to(device)\n",
    "    # vision_model.eval()\n",
    "    # for param in vision_model.parameters():\n",
    "    #     param.requires_grad = False\n",
    "else:\n",
    "    print(f\"Model file not found at {load_path}. Cannot load model.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 6: Conclusion\n",
    "\n",
    "This notebook demonstrated a highly simplified, inline approach to text-to-image generation using a Transformer architecture. Instead of generating raw pixels, we adapted the pre-trained text Transformer to predict an *image feature vector* corresponding to a textual description.\n",
    "\n",
    "Key steps involved:\n",
    "1.  **Loading:** Reusing text Transformer components and the ResNet feature extractor from the previous multi-modal model.\n",
    "2.  **Data:** Defining text prompts paired with target images and pre-extracting the *target* image feature vectors using the frozen ResNet.\n",
    "3.  **Architecture Adaptation:** Replacing the final output layer of the Transformer with a new linear layer projecting hidden states to the image feature dimension.\n",
    "4.  **Training:** Training the Transformer components and the new output layer to minimize the Mean Squared Error (MSE) between the *predicted* image feature vector and the *target* image feature vector for given text prompts.\n",
    "5.  **Generation & Simplified Reconstruction:** Using the trained model to predict a feature vector from a new text prompt. Then, finding the training image whose actual feature vector is closest (using Euclidean distance) to the predicted vector and displaying that image as the result.\n",
    "\n",
    "This method illustrates the concept of using Transformers for cross-modal generation (text to a visual representation) while adhering to the inline implementation constraint. It avoids the significant complexity of direct pixel generation (GANs, Diffusion) but provides a tangible, albeit very basic, visual output by leveraging nearest-neighbor search in the feature space of known images. Real-world text-to-image models are vastly more complex, often combining different architectures and trained on enormous datasets."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv-multimodaal",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
